Modelling meaning: a short history of text markup

Lou Burnard

Not so long ago (about 30 years ago to be precise), it was easy to
decide how to process text by computer. If you wanted to store and
then print the text out nicely, you would use one kind of software. If
you wanted to analyse the text linguistically (yes, people did that 30
years ago too), you'd use another. And if you were really interested
in what the text was *about*, then you would probably throw the text
away and use a database. But then someone realised that actually it
might be more fun to combine all three kinds of process -- dealing
with the text as an image, as a linguistic construct, and as a body of
assertions about the real world -- and the need for a new kind of
markup was born. In this talk I'll give a biassed and unreliable
account of some key moments in the evolution of our current markup
systems: from the corporate database of the 1970s to today's world
wide web of documents and tomorrow's web of data, by way of SGML, TEI,
XML, and other acronyms. Time permitting, I'll also try to explain why
data and text are not so different after all.

COMPONENTS OF A DOCUMENT
content: the components (words, images etc). which make up a document
structure: the organization and inter-relationship of  the components
presentation: how a document looks and what processes are applied to it

SEPARATION MEANS
the content can be re-used
the structure can be formally validated
the presentation can be customized for
different media
different audiences
… in short, the information can be uncoupled from its processing
This is not a new idea! But it’s a good one...


<!-- see tei-fr/Talks/2015-01-warsaw/tei-history.xml-->

<div><head>1987 was a long time ago...</head>
<p>The Text Encoding Initiative was born into a very different world
<list rend="pause">
<item>the world wide web did not exist</item>
<item>the tunnel beneath the English Channel was still
being built</item>
<item>a state called the Soviet Union had just launched a space
station called Mir</item>
<item>serious computing was done on mainframes </item>
<item>mobile phones  did not exist</item>
</list></p></div>

<div><head>...but also a familiar one</head>
<list>
<item>Corpus linguistics and <q>artificial intelligence</q> 
had created a demand for large scale lexical
resources in academia and beyond</item>
<item>Advances in text processing were beginning to affect
lexicography and document management systems  (e.g. TeX, Scribe,
tRoff..)</item>
<item>The Internet existed and theories about how to use it
<q>hypertextually</q> abounded </item>
<item>Books, articles, and even courses in something called
"Computing in the Humanities" were becoming commonplace</item>
</list>

</div>

<div><head>Birth of the Text Encoding Initiative</head>
<list>
<item>Spring 1987: European workshops on standardisation of historical data (J.P
.
Genet, M. Thaller )</item>
<item>Autumn 1987: In the US, the NEH funds an exploratory international worksho
p on the
feasibility of defining "text encoding guidelines"</item>
</list>
<figure>
<graphic height="70%" url="../Graphics/poughkeepsie.png"/>
<head>Vassar College, Poughkeepsie</head>
</figure>
</div>


<div><head>Today's question:</head>
<list>
<item>So the TEI is <emph>very old</emph>!</item>
<item>It comes from a time before the Web, before the DVD, the mobile
phone, cable tv, or Microsoft Word</item>
<item>Not much in computing survives 5 years, never mind 20</item>
<item>Why is it still here, and how has it survived?</item>
<item>What relevance can it possibly have today?</item>
</list>
</div>

