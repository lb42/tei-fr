<?xml version="1.0" encoding="utf-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0">
   <teiHeader>
      <fileDesc>
         <titleStmt>
            <title>Untitled Document</title>
            <author/>
         </titleStmt>
         <editionStmt>
            <edition>
               <date>2017-04-23T17:18:45.636837364</date>
            </edition>
         </editionStmt>
         <publicationStmt>
            <p>no publication statement available</p>
         </publicationStmt>
         <sourceDesc>
            <p>Written by OpenOffice</p>
         </sourceDesc>
      </fileDesc>
      <revisionDesc>
         <listChange>
            <change>
               <name/>
               <date>2017-04-23T17:21:04.437686625</date>
            </change>
         </listChange>
      </revisionDesc>
   </teiHeader>
   <text>
      <body>
         <p rend="center color(#000000)">
            <anchor xml:id="id_docs-internal-guid-72be745a-9b9c-5bf3-6485-187b078dbbd6"/>Digital
            Classics III: Re-thinking Text Analysis</p>
         <div type="div1" rend="P16">
            <head>Program</head>
            <div type="div2" rend="P2">
               <head>Thursday, May 11</head>
               <p>13:15 Opening (Chronopoulos-Maier-Novokhatko)</p>
               <p>
                  <hi>13:30</hi>
                  <hi> </hi>
                  <hi>James Brusuelas (Oxford): </hi>
                  <ref
                     target="https://docs.google.com/document/d/1b8oPWy5Pww_ICW3e_YbKXE4jh1Vh9HFG1fzWfZRajaw/edit#heading=h.owbyywlktbt7">
                     <hi rend="color(#1155cc)underline italic">Neural Classics? The wonders and
                        problem of automation</hi>
                  </ref>
               </p>
               <p>
                  <hi>14:15</hi>
                  <hi> </hi>
                  <hi>Alek Keersmaekers &amp; Mark Depauw (Leuven) </hi>
                  <ref
                     target="https://docs.google.com/document/d/1b8oPWy5Pww_ICW3e_YbKXE4jh1Vh9HFG1fzWfZRajaw/edit#heading=h.9ujjw0d5hml8">
                     <hi rend="color(#1155cc)underline italic">Bringing together Linguistics and
                        Social History in automated Text Analysis of Greek papyri</hi>
                  </ref>
               </p>
               <p>
                  <hi>15:00</hi>
                  <hi> </hi>
                  <hi>Marja Vierros (Helsinki): </hi>
                  <ref
                     target="https://docs.google.com/document/d/1b8oPWy5Pww_ICW3e_YbKXE4jh1Vh9HFG1fzWfZRajaw/edit#heading=h.kvzdqwq2outz">
                     <hi rend="color(#1155cc)underline italic">Papyri and digital linguistic
                        analysis: building the Sematia corpus and platform</hi>
                  </ref>
               </p>
               <p>15:45 Coffee break</p>
               <p>
                  <hi>16:15</hi>
                  <hi> </hi>
                  <hi>Jan Christoph Meister (Hamburg): </hi>
                  <ref
                     target="https://docs.google.com/document/d/1b8oPWy5Pww_ICW3e_YbKXE4jh1Vh9HFG1fzWfZRajaw/edit#heading=h.mi90f61ic627">
                     <hi rend="color(#1155cc)underline italic">Digitizing the hermeneutic circle:
                        Parameterizing 'context' in hermeneutic text annotation</hi>
                  </ref>
                  <hi> </hi>
               </p>
               <p>
                  <hi>17:00</hi>
                  <hi> </hi>
                  <hi>Michèle Brunet (Lyon): </hi>
                  <ref
                     target="https://docs.google.com/document/d/1b8oPWy5Pww_ICW3e_YbKXE4jh1Vh9HFG1fzWfZRajaw/edit#heading=h.1nkshcbrkksc">
                     <hi rend="color(#1155cc)underline italic">How to deal correctly with
                        inscriptions in a Digital World?  Modelization for Semiotic Analysis</hi>
                  </ref>
               </p>
               <p>
                  <hi>18:30 Plenary talk: Milad Doueihi (Paris): </hi>
                  <ref
                     target="https://docs.google.com/document/d/1b8oPWy5Pww_ICW3e_YbKXE4jh1Vh9HFG1fzWfZRajaw/edit#heading=h.qlyweywak5ze">
                     <hi rend="color(#1155cc)underline italic">Digital materialism</hi>
                  </ref>
               </p>
               <p>
                  <hi>20:00 Conference dinner</hi>
                  <hi> </hi>
               </p>
            </div>
            <div type="div2" rend="P2">
               <head>Friday, May 12</head>
               <p>
                  <hi>9:00</hi>
                  <hi> </hi>
                  <hi>Bénédicte Pincemin (Lyon): </hi>
                  <ref
                     target="https://docs.google.com/document/d/1b8oPWy5Pww_ICW3e_YbKXE4jh1Vh9HFG1fzWfZRajaw/edit#heading=h.gatmsuehkvof">
                     <hi rend="color(#1155cc)underline italic">Introduction to textometric
                        methodology</hi>
                  </ref>
               </p>
               <p>
                  <hi>9:45 Tariq Yousef (Leipzig): </hi>
                  <ref
                     target="https://docs.google.com/document/d/1b8oPWy5Pww_ICW3e_YbKXE4jh1Vh9HFG1fzWfZRajaw/edit#heading=h.us96onqku95">
                     <hi rend="color(#1155cc)underline italic">Creating Dynamic Lexica through
                        Bridge Languages</hi>
                  </ref>
               </p>
               <p>10:30 Coffee break</p>
               <p>
                  <hi>11:00</hi>
                  <hi> </hi>
                  <hi>Erik Henriksson (Helsinki): </hi>
                  <ref
                     target="https://docs.google.com/document/d/1b8oPWy5Pww_ICW3e_YbKXE4jh1Vh9HFG1fzWfZRajaw/edit#heading=h.et39uabjwxda">
                     <hi rend="color(#1155cc)underline italic">A New Research Toolkit and Database
                        for Greek Metrics</hi>
                  </ref>
               </p>
               <p>
                  <hi>11:45</hi>
                  <hi> </hi>
                  <hi>Margherita Fantoli, Dominique Longrée, Marc Vandersmissen (Liege), Laurent
                     Vanni (Nizza): </hi>
                  <ref
                     target="https://docs.google.com/document/d/1b8oPWy5Pww_ICW3e_YbKXE4jh1Vh9HFG1fzWfZRajaw/edit#heading=h.8vwx14tqs426">
                     <hi rend="color(#1155cc)underline italic">Lexicometry with the new website
                        Hyperbase Greek: the case study of the Greek tragedy – Euripides compared to
                        Seneca</hi>
                  </ref>
               </p>
               <p>12:30 Lunch break</p>
               <p>
                  <hi>14:30</hi>
                  <hi> </hi>
                  <hi>Timo Korkiakangas (Oslo): </hi>
                  <ref
                     target="https://docs.google.com/document/d/1b8oPWy5Pww_ICW3e_YbKXE4jh1Vh9HFG1fzWfZRajaw/edit#heading=h.z84nnhcqp511">
                     <hi rend="color(#1155cc)underline italic">Measuring spelling variation and
                        morphosyntactic competence: a methodological answer to a question never
                        before asked for Latin</hi>
                  </ref>
               </p>
               <p>
                  <hi>15:15</hi>
                  <hi> </hi>
                  <hi>Yves Ouvrard and Philippe Verkerk (Lille): </hi>
                  <ref
                     target="https://docs.google.com/document/d/1b8oPWy5Pww_ICW3e_YbKXE4jh1Vh9HFG1fzWfZRajaw/edit#heading=h.fxzj1e80ymbe">
                     <hi rend="color(#1155cc)underline italic">Collatinus: Lemmatizer and
                        morphological analyzer for Latin texts</hi>
                  </ref>
                  <hi> </hi>
               </p>
               <p>16:00 Coffee break</p>
               <p>
                  <hi>16:30</hi>
                  <hi> </hi>
                  <hi>Neel Smith, Charles Schufreider, Melody Wauke ( College of the Holy Cross): </hi>
                  <ref
                     target="https://docs.google.com/document/d/1b8oPWy5Pww_ICW3e_YbKXE4jh1Vh9HFG1fzWfZRajaw/edit#heading=h.q23tzg767c5m">
                     <hi rend="color(#1155cc)underline italic">Recovering the history of Iliadic
                        scholia: architecture and initial results from the Homer Multitext project
                        (HMT)</hi>
                  </ref>
               </p>
               <p>
                  <hi>17:15</hi>
                  <hi> </hi>
                  <hi>Francesco Mambrini (Berlin): </hi>
                  <ref
                     target="https://docs.google.com/document/d/1b8oPWy5Pww_ICW3e_YbKXE4jh1Vh9HFG1fzWfZRajaw/edit#heading=h.elwwh0r779xs">
                     <hi rend="color(#1155cc)underline italic">Idiolects of the Heroes? Towards a
                        treebank-based approach to the language of the Sophoclean characters</hi>
                  </ref>
                  <hi> </hi>
               </p>
               <p>
                  <hi>18:30</hi>
                  <hi> </hi>
                  <hi>Plenary talk: Lou Burnard (Oxford): </hi>
                  <ref
                     target="https://docs.google.com/document/d/1b8oPWy5Pww_ICW3e_YbKXE4jh1Vh9HFG1fzWfZRajaw/edit#heading=h.mlrjoh2eqsnm">
                     <hi rend="color(#1155cc)underline italic">What is TEI conformance, and why
                        should you care?</hi>
                  </ref>
               </p>
            </div>
            <div type="div2" rend="P1">
               <head>Saturday, May 13</head>
               <p>
                  <hi>8:45</hi>
                  <hi> </hi>
                  <hi>Charlotte Schubert (Leipzig): </hi>
                  <ref
                     target="https://docs.google.com/document/d/1b8oPWy5Pww_ICW3e_YbKXE4jh1Vh9HFG1fzWfZRajaw/edit#heading=h.elwwh0r779xs">
                     <hi rend="color(#1155cc)underline italic">Editing and Un-editing in Digital
                        Classics</hi>
                  </ref>
               </p>
               <p>
                  <hi>9:30</hi>
                  <hi> </hi>
                  <hi>Gregory Crane (Tufts/Leipzig): </hi>
                  <ref
                     target="https://docs.google.com/document/d/1b8oPWy5Pww_ICW3e_YbKXE4jh1Vh9HFG1fzWfZRajaw/edit#heading=h.e64x3glcse37">
                     <hi rend="color(#1155cc)underline italic">Greek and Latin in an Age of massive
                        collections and global philology</hi>
                  </ref>
               </p>
               <p>
                  <hi>10:15</hi>
                  <hi> </hi>
                  <hi>Rodney Ast (Heidelberg): </hi>
                  <ref
                     target="https://docs.google.com/document/d/1b8oPWy5Pww_ICW3e_YbKXE4jh1Vh9HFG1fzWfZRajaw/edit#heading=h.3bctxwfo8mi">
                     <hi rend="color(#1155cc)underline italic">Can the digital humanities make us
                        better humanists? A case study in Papyrology</hi>
                  </ref>
               </p>
               <p>11:00 Coffee break</p>
               <p>
                  <hi>11:30</hi>
                  <hi> </hi>
                  <hi>Neven Jovanović (Zagreb): </hi>
                  <ref
                     target="https://docs.google.com/document/d/1b8oPWy5Pww_ICW3e_YbKXE4jh1Vh9HFG1fzWfZRajaw/edit#heading=h.gpjmequyttr9">
                     <hi rend="color(#1155cc)underline italic">Atomic philology and parallel
                        philology – some implications of the CITE architecture</hi>
                  </ref>
               </p>
               <p>
                  <hi>12:15</hi>
                  <hi> </hi>
                  <hi>James Cowey (Heidelberg): </hi>
                  <ref
                     target="https://docs.google.com/document/d/1b8oPWy5Pww_ICW3e_YbKXE4jh1Vh9HFG1fzWfZRajaw/edit#heading=h.hc1s2wm03xl">
                     <hi rend="color(#1155cc)underline italic">Challenges and opportunities for the
                        discipline of Papyrology in the age of digital editions</hi>
                  </ref>
               </p>
               <p>
                  <hi>13:00</hi>
                  <hi> </hi>
                  <hi>Nicola Reggiani (Trier/Parma): </hi>
                  <ref
                     target="https://docs.google.com/document/d/1b8oPWy5Pww_ICW3e_YbKXE4jh1Vh9HFG1fzWfZRajaw/edit#heading=h.rae0qrh4rsew">
                     <hi rend="color(#1155cc)underline italic">The digital edition of ancient
                        sources as a further step in the textual transmission</hi>
                  </ref>
               </p>
               <p>
                  <hi>13:45</hi>
                  <hi> </hi>
                  <hi>Conclusion and buffet</hi>
               </p>
               <p>
                  <lb/>
                  <hi></hi>
                  <lb/>
                  <hi></hi>
               </p>
            </div>
         </div>
         <div type="div1" rend="P15">
            <head>Abstracts</head>
            <div type="div2" rend="P3">
               <head>
                  <hi>James Brusuelas: </hi>
                  <hi>Neural Greek? The wonders and problem of automation</hi>
               </head>
               <p>Machine learning and artificial intelligence is already a part of daily life.
                  Smartphones and applications created by Google and Facebook employ sophisticated
                  image recognition algorithms for social interaction and search purposes. But it is
                  the robust artificial intelligence employed in manufacturing and now even in
                  self-driving vehicles that is often the subject of news headlines. Automation is
                  very much a contentious and problematic topic in the context of jobs and the
                  global economy. But replacing the human actor with an artificial intelligence is
                  not solely an economic issue. Digital Humanities too has already begun to explore
                  how machine or deep learning can be applied to text analysis. In doing so, the
                  typical tasks of human scholars in manuscript analysis are part of the process of
                  replication and ultimately automation. But many aspects of Digital Humanities work
                  also rely on a human actor for annotation, whether that is the creation of an XML
                  file for a digital edition or the act of treebanking. The integration of what is
                  sometimes even referred to as a weak artificial intelligence has potential impact
                  not only on scholarly acts in Humanities but also in the Digital Humanities
                  itself. The purpose of this paper is not only to discuss aspects of automation as
                  it has been applied to text analysis, particularly in the Ancient Lives and
                  Proteus projects, but also to explore how deep neural networks can and will be
                  applied to extracting and analysing text data directly from manuscript images.</p>
            </div>
            <div type="div2" rend="P3">
               <head>
                  <hi>K. Keersmaekers and Mark Depauw: </hi>
                  <hi>Bringing together Linguistics and Social History in automated Text Analysis of
                     Greek papyri</hi>
               </head>
               <p>This paper presents the ongoing linguistic and historical annotation of Greek
                  documentary papyrus texts, in the framework of a collaboration between the KU
                  Leuven linguistics department and the (social-historical) Trismegistos
                  project.</p>
               <p>As a first step, we have tokenized all texts from the Duke Databank of Documentary
                  Papyri, distinguishing the original words of the scribe from the editorial
                  regularizations in the process. The tokenization is not only essential for the
                  further automated linguistic processing of these texts, but will also help
                  historians in the Trismegistos project to speed up the annotation of certain words
                  and phrases (e.g. people, place names, dates).</p>
               <p>In a second stage, these tokenized texts can be automatically analyzed with
                  Natural Language Processing (NLP) methods. We have developed a part-of-speech
                  tagging system, using an open-source part-of-speech-tagger trained on classical
                  and post-classical Greek prose, which produces fairly good results when applied to
                  a test corpus of papyrus texts (ca. 94% accuracy). Other NLP techniques, including
                  syntactic parsing, text genre classification and native-language identification,
                  are also being explored. These processes profit from data collected in several
                  historical and linguistic Trismegistos databases, including People and Places
                  (which respectively include all given names and place names in papyri) and Text
                  Irregularities (which classifies editorial regularizations).</p>
               <p>In the end, the linguistic annotations will be used to perform corpus linguistic
                  analyses of the papyri, including research into language variation. Their use in
                  historical research, in turn, will be illustrated through some case studies.</p>
            </div>
            <div type="div2" rend="P3">
               <head>
                  <hi>Marja Vierros: </hi>
                  <hi>Papyri and digital linguistic analysis: building the Sematia corpus and
                     platform</hi>
               </head>
               <p>
                  <hi>Greek and Latin documentary papyri form an important source material for the
                     historical development and linguistic variation present in postclassical Greek
                     and Latin used in Egypt, respectively. The texts are often preserved in
                     fragmentary condition, which is why the texts have been difficult to analyse or
                     annotate with digital corpus linguistic tools. We have created one solution for
                     preprocessing the papyrus texts for linguistic annotation. During the process
                     we take into account the information coded in the digital version of the papyri
                     in TEI EpiDoc XML and transform the text into two layers which can both be
                     uploaded to an annotation platform for morphosyntactic annotation (according to
                     the Ancient Greek and Latin Dependency Treebank Guidelines). These layers and
                     their corresponding treebanks will eventually form the Sematia corpus and
                     platform, which is already in progress and openly available in </hi>
                  <ptr target="https://sematia.hum.helsinki.fi/"/>
                  <hi>. We are currently developing further features, especially focusing on
                     variation and on querying. In this paper I will describe the preprocessing and
                     choices made within, as well as discuss the future directions and needs.</hi>
               </p>
               <p>Reference:</p>
               <p>
                  <hi>Vierros, M. and E. Henriksson (forthcoming), ”Preprocessing Greek Papyri for
                     Linguistic Annotation” in Computer-Aided Processing of Intertextuality in
                     Ancient Languages. Journal of Data Mining and Digital Humanities. Edited by M.
                     Büchler and L. Mellerin.  Preprint in open archive HAL:</hi>
                  <ref target="https://hal.archives-ouvertes.fr/hal-01279493">
                     <hi rend="color(#1155cc)underline"> </hi>
                  </ref>
                  <ptr target="https://hal.archives-ouvertes.fr/hal-01279493"/>
                  <hi> </hi>
               </p>
            </div>
            <div type="div2" rend="P3">
               <head>
                  <hi>Jan Christoph Meister: </hi>
                  <hi>Digitizing the hermeneutic circle: Parameterizing 'context' in hermeneutic
                     text annotation</hi>
               </head>
               <p>The 'distant reading' of large text corpora can, as we have seen, produce highly
                  interesting insights into the development of historical forms, genres, styles,
                  themes etc. over time. But it is a different type of knowledge altogether: The
                  traditional historical and hermeneutic approach in literary studies was
                  characterized by the 'close reading' based in-depth analysis and interpretation of
                  a small set of texts that were considered exemplary and often taken from an
                  established, normative literary canon. Most of the new digital methods trade the
                  hermeneutic precision and ingenuity of 'closeness' to the exemplary for the
                  statistical generalizablity of results that are derived from the perspective of
                  methodological 'distance' and of disinterest in the highly original or
                  idiosyncratic artefact. </p>
               <p>However, one can argue that this is only in part a conscious methodological
                  choice. Obviously the analysis of really large text corpora is also subject to
                  rather pragmatic constraints – we simply do not have the time and resources to
                  process large numbers of texts 'manually' – or rather: intellectually, that is, by
                  way of human intelligence controlled processes of analysis, categorization,
                  interpretation, critique and validation, etc. .  In response to these pragmatic
                  constraints digital corpus analysis therefore relies heavily on data mining
                  techniques, natural language processing technology and statistical pattern
                  analysis. All these methods prove extremely powerful the more they can ignore the
                  contextually induced specificity of the phenomena under investigation - such as,
                  the historicity of a particular language variant, the genre specificity of a
                  topic, etc..</p>
               <p>The exploration of context and its impact on meaning is and still remains the
                  forte of hermeneutic and not of statistical investigation, because in hermeneutics
                  it is the specificity of constellations that counts, and not their
                  generalizability. One way to resolve what at first sight may look like a
                  principled methodological impasse between the 'close' and hermeneutic and the
                  'distant' and statistical methods of text studies is to enrich digital text
                  analysis with one of the oldest, and probably the most powerful method of
                  traditional text studies: annotation. In my contribution I will demonstrate how
                  the use of C ATMA, a digital text annotation platform, can help us to bridge the
                  gap between the hermeneutic and the statistical approach to literary phenomena. In
                  CATMA this is achieved by conceptualizing the context dependency of literary
                  phenomena as a type of 'parameterization' - an approach which, in the end, might
                  even enable us to model the hermeneutic circle in a 'digital' way.</p>
            </div>
            <div type="div2" rend="P3">
               <head>
                  <hi>Michèle Brunet: </hi>
                  <hi>How to deal correctly with inscriptions in a Digital World?  Modelization for
                     Semiotic Analysis</hi>
               </head>
               <p>Traditionally, an epigraphical edition is limited to a critical edition of what we
                  call a « text », without considering what are the distinctive feature of such a
                  text : its materiality, that is to say its lettering and its physical display on
                  the stone support, but also the many links interweaving the message expressed with
                  words (the text) to other meanings expressed by the images, figurative or not,
                  which are the other components of the inscribed monument. </p>
               <p>In a digital world, many new possibilities occur for scholars interested in
                  semiotics of monuments bearing inscriptions (regardless of the period they belong
                  to). One can now look at them considering what they are really, global
                  communication projects in order to publish them accordingly to this definition.
                  Drawing on research conducted as part of my publishing program of the collection
                  of the Greek Inscriptions of the Louvre, I propose to present a paper about the
                  analytical process leading up to a global modelization of an inscribed monument in
                  order to show how this process can give way to a digital publication where the
                  edition of the epigraphical text in EpiDoc/TEI/XML can be effectively related, on
                  both a semantic and a technical point of view, to all the other components of the
                  monument which are stakeholders of the communication device as well as the text
                  itself.</p>
            </div>
            <div type="div2" rend="P3">
               <head>
                  <hi>Milad Doueihi: </hi>
                  <hi>Digital materialism</hi>
               </head>
               <p>
                  <hi>If, according to W. Benjamin, our encounter with the </hi>
                  <hi>automaton</hi>
                  <hi> </hi>
                  <hi>leads to a confrontation with the relations between historical materialism and
                     theology, what is Digital materialism that has emerged since the 1950s? How
                     does Digital materialism, with its intricate infrastructure and the ways in
                     which it has reshaped not only our knowledge economy but also the social,
                     differ from previous materialisms? And what are the consequences for such a
                     reshaping of our intellectual landscape for the Humanities?</hi>
               </p>
            </div>
            <div type="div2" rend="P3">
               <head>
                  <hi>Bénédicte Pincemin: </hi>
                  <hi>Introduction to textometric methodology – for Humanists who would like not to
                     choose between digital edition, database and search engine to study and publish
                     their text corpora</hi>
               </head>
               <p>Textometry can be seen as a relevant approach for Humanities research, as analysis
                  is directly grounded into texts (and not necessarily mediated by external
                  linguistic or semantic resources), and as the researcher keeps all the
                  investigations under control. </p>
               <p>Textometry combines quantitative and qualitative processing. Statistics or basic
                  heuristics are defined which make sense on textual data (specificities and
                  cooccurrence scores computed through an exact Fisher test ; map visualization of
                  the corpus based on correspondence analysis). The "back-to-text" functionality is
                  the core of textometric processing: every result (lexical list, table or graph)
                  must be interpreted with a look to corresponding words in context.</p>
               <p>In such an analytic framework, text editions can still meet philologic
                  requirements : TXM software for example can work on TEI encoded corpora, show fine
                  HTML editions, and manage several aligned versions of the same text, including
                  multimedia recording of the original source (for instance pictures of
                  manuscripts).</p>
               <p>A typology of textometric investigations will be sketched :</p>
               <p>(i) querying about a word's occurrences, with a statistical evaluation of its
                  frequency (the query can also target a phrase, or a morphosyntactic pattern, or
                  any complex combination of linguistic features available in the corpus, with
                  possible gaps) ;</p>
               <p>(ii) looking for a word's (or phrase's etc.) contextual use and meaning in the
                  current corpus or in some part of it ;</p>
               <p>(iii) computing paradigmatic series, for example listing all the lexical units
                  occurring in some precise position (for instance words ending verses, or
                  adjectives qualifying a given noun) ;</p>
               <p>(iv) contrastive analysis of a corpus : specific vocabulary of periods, or
                  authors, or text types, etc., and inferring and visualizing of the main dimensions
                  of contrast structuring the corpus ;</p>
               <p>(v) for diachronic corpora, overall view of evolution of words or linguistic
                  features.</p>
               <p>These five types of corpus querying will be illustrated with the use of TXM
                  open-source software, which is available for multiple operating systems (including
                  a web portal version), most languages (including latin and greek), and many corpus
                  encoding states (from raw text corpora to TEI encoded ones)
                  (http://textometrie.ens-lyon.fr).</p>
            </div>
            <div type="div2" rend="P3">
               <head>
                  <hi>Tariq Yousef: </hi>
                  <hi>Creating Dynamic Lexica through Bridge Languages</hi>
               </head>
               <p>Bilingual lexicons are important resources for machine translation and automatic
                  alignment systems. This paper presents a simple and effective model for extracting
                  bilingual lexicons automatically from pre-aligned parallel texts by using
                  information retrieval techniques. The model is based on the assumption that two
                  words/phrases are likely to be translations if they are aligned to the same
                  word/phrase in a third language.</p>
               <p>This work aims to create dynamic lexicons and training data for machine
                  translation and automatic alignment systems for less-known language pairs (e.g
                  Persian-Ancient Greek).  The accuracy of these lexicons relies on the alignment
                  quality, the number of bridge languages, therefore manual aligned texts are
                  supposed to produce more accurate lexicons than the automatic aligned texts. The
                  aligned parallel texts size (i.e. number of aligned tokens) determines the size of
                  the extracted lexicon, larger aligned texts yield larger number of lexicon
                  entries.</p>
               <p>As a use case we used the  Greek-English, Latin-English,  Persian-English aligned
                  parallel texts available in Perseus Digital Library to produce Greek-Latin,
                  Greek-Persian and  Latin-Persian dynamic lexicons.</p>
            </div>
            <div type="div2" rend="P3">
               <head>
                  <hi>Erik Henriksson: </hi>
                  <hi>A New Research Toolkit and Database for Greek Metrics</hi>
               </head>
               <p>In the field of ancient Greek metrics, data collection and analysis still depends
                  on manual annotation. There are no online databases of metrically annotated Greek
                  poems and very few open source tools that work with such data. This is perhaps
                  partly explained by the fact that metrical analysis is relatively difficult to
                  automatize; moreover, some scholars believe that there is little room left for
                  more discoveries in Greek metrics. Especially in linguistic branches of metrics
                  that focus on the connections between language and verse (such as generative
                  metrics), however, new digital research methods and tools are acutely needed. This
                  paper will present one attempt at creating such a toolkit.</p>
               <p>The database includes texts from Homer to early Byzantine poetry, collected from
                  various online databases and printed editions. The metrical parser, written in
                  Python, is based on a templatic theory of metrics and the notion that meter is
                  about similarity between the template and its prosodic realization. The system
                  generates all possible templates of known meters (excluding lyric meters) as well
                  as every legitimate syllabification of the input line, and matches them against
                  each other. The optimal reading, i.e. the match with least constraint violations
                  and greatest prosodic similarity, is then stored in the database as a metrical
                  annotation; these, in turn, can later be queried for metrical properties like
                  bridges, caesuras or positions of accents across the templates. At the time of
                  writing, the database holds 50 000 metrical annotations.</p>
               <p>The toolkit, though still in development, is intended to open up new ways to study
                  metrical texts both statistically and in close reading. At its simplest, it might
                  be used to count the relative proportions of spondees and dactyls in Homer; but
                  contemporary metrics obviously deals with far more complex issues. A metrical
                  line, in fact, can be analysed as a hierarchic organization of prosodic
                  constituents that go beyond word or feet boundaries. A partly unsolved challenge,
                  discussed at the end of the paper, is the automatic detection of these
                  higher-level metrical units.</p>
            </div>
            <div type="div2" rend="P4">
               <head>
                  <hi>Margherita Fantoli, Dominique Longrée, Marc Vandersmissen, Laurent Vanni: </hi>
                  <hi>Lexicometry with the new website Hyperbase Greek: the case study of the Greek
                     tragedy – Euripides compared to Seneca</hi>
               </head>
               <p>The LASLA database of Greek Classical literary texts has until now been too rarely
                  used for linguistic or literary researches. This underuse results probably from
                  the fact that the LASLA Greek files were until now not accessible on line: for the
                  founders of the LASLA, the main objective of these files was to produce new
                  indexes of literary texts: some were published in the well-known series
                  Alpha-Omega (Olms), but others in the more confidential series of the CIPL. So
                  until very recently the only way to get data from those files was to ask the LASLA
                  to make the needed research.</p>
               <p>Another reason for the underuse of the LASLA Greek database in regard with the
                  LASLA Latin databases is probably that the LASLA Greek files provide less
                  morphological or syntactical annotations than the Latin one. Nevertheless these
                  files constitute a big set of lemmatized annotated texts, each word forms being
                  annotated also with a Part of Speech tag. In addition, all the lemmas and the
                  annotations have been selected and verified by a trained philologist (see C.
                  Denizot et M. Vandersmissen, Guide de lemmatisation des textes grecs: Théorie et
                  pratique, e-print, 2013, http://hdl.handle.net/2268/167789). This very demanding
                  choice of a human systematic verification explains why, in comparison with others
                  Greek databases, the LASLA database could seem very small (only a little bit more
                  than 1.200.000 word forms), but it includes however main Greek literary works as
                  the whole tragic corpus, or several works of Aristotle and Plato. This database
                  has already proved itself to be of a real interest for philologists, linguists and
                  historians.</p>
               <p>In collaboration with the UMR 7320, “Bases, corpus, langage” –CNRS – University of
                  Nice Sophia Antipolis, the LASLA decided recently to make its Greek database
                  available through the online software Hyperbase Web edition, as the Latin database
                  already is. This software allowed both documentary and statistical researches.
                  With Hyperbase Web, it is now possible to create not only concordances based on
                  word forms but also on lemmas or on part of speeches, with the possibility to
                  combine word forms, lemmas, POS annotations and gaps between items in complex
                  expressions. But the main interest of Hyperbase Web is to allow statistical
                  investigations. The software shows automatically all the specific word forms,
                  lemmas or POS of the considered text in regard with the other texts of the
                  database. It allows also to search for the distribution of word forms, lemmas, POS
                  or complex expressions in each text of the corpus, as well as to search for
                  collocates of the same items, both in a particular text or in the whole corpus.
                  The Greek Hyperbase, as well as the Latin Hyperbase, are therefore, powerful
                  instruments for lexicometric analysis. The research potential of Hyperbase will be
                  exemplified by a comparison between the lexicons of Euripides and Seneca. Making
                  use of statistical devices, as the Tree Analysis or the Correspondences Analysis,
                  we will try to sort out in which way the distribution of the vocabulary is mainly
                  conditioned by the writer or by the thematic of the play.</p>
            </div>
            <div type="div2" rend="P3">
               <head>
                  <hi>Timo Korkiakangas: </hi>
                  <hi>Measuring spelling variation and morphosyntactic competence: a methodological
                     answer to a question never before asked for Latin</hi>
               </head>
               <p>This paper presents a spin-off study which highlights the possibilities a
                  well-built corpus often provides beyond what the corpus was originally intended
                  for. The here presented method of quantifying spelling correctness by way of word
                  form normalisation and edit-distances is an example of how digital text analysis
                  answers questions previously undreamed-of because of their laboriousness. The
                  method opens a unique window into the language competences of individual Latin
                  writers and can be exploited by historical linguistics to draw conclusions on the
                  use of spoken language in diachrony (for the use of spelling correctness in
                  linguistics, see e.g. Burt 2006, Wood &amp; al. 2011). This corpus-based
                  data-driven method can be extended to any lemmatised and morphologically tagged
                  corpus of any language variant with spelling variation in respect of the obtaining
                  orthographical norm.</p>
               <p>I will investigate whether the considerable oscillation in spelling in the Latin
                  of early medieval private documents (charters) correlates with the oscillation in
                  employing certain morphosyntactic categories. My hypothesis is that the more
                  non-standard the spelling is, the more frequent are the novel, Romance-type,
                  morphosyntactic constructions and the poorer the command of those Classical
                  constructions that were in decline in Late Latin. The Romance-type constructions
                  had obviously crept into the written code from the spoken idiom of the time, while
                  the conservative constructions derived from the centuries-old legal Latin. These
                  were learnt (or not) through training and reproduced with varying success
                  (Sabatini 1965). Importantly, the here developed spelling correctness metrics can
                  be used to examine the linguistic status of features which have so far not allowed
                  categorisation according to whether they are to be considered evanescent or still
                  typical of the spoken usage: if the decreasing or hypercorrective use of a certain
                  feature is systematically associated with non-standard spelling, the feature is
                  likely to have been alien to the spoken language of the time.</p>
               <p>Spelling variation is here operationalised by normalising the non-standard word
                  forms into standard-Latin forms and subsequently quantified by calculating the
                  edit-distance between all the word forms of the corpus and their standard-Latin
                  counterparts, whether these be originally standard or non-standard. Thus, each
                  word form of the corpus receives a value which indicates the word's distance from
                  the respective standard form. This spelling correctness metrics can be then
                  calculated for individual scribes, locations, or periods of time. I will present
                  two case studies which illustrate the use of the metrics with metadata and
                  selected linguistic phenomena. The concept of 'standard' as well as the
                  possibility of weighting different types of spelling deviations will also be
                  discussed.</p>
               <p>
                  <hi>The normalisation of word forms is based on the two-million-word Open Office
                     Latin lexicon which I lemmatised and tagged morphologically with Whitaker's
                     WORDS tagger. The resulting word form library consists of one or more
                     lemma/morphology entries for each word form. This library was then used to
                     produce Classical-Latin forms out of the lemma/morphology pairs attached to
                     each word in the Late Latin Charter Treebank (LLCT, 480,000 words, Korkiakangas
                     &amp; Passarotti 2011), which consists of 1,040 charters from Tuscany of the
                     8</hi>
                  <hi>th</hi>
                  <hi> </hi>
                  <hi>and 9</hi>
                  <hi>th</hi>
                  <hi> </hi>
                  <hi>centuries.</hi>
               </p>
               <p>References:</p>
               <p>Burt, J. 2006. 'Spelling in Adults: The Combined Influences of Language Skills and
                  Reading Experience', in Journal of Psycholinguist Research, 35(5), 447–470.</p>
               <p>Korkiakangas, T. &amp; Passarotti, M. 2011. 'Challenges in Annotating Medieval
                  Latin Charters', in Journal of Language Technology and Computational Linguistics
                  (JLCL) 26:2, 103–114. Heidelberg, 2012.</p>
               <p>LLCT = Late Latin Charter Treebank. Available from the author upon request.</p>
               <p>
                  <hi>Open Office Latin spelling and hyphenation dictionaries:</hi>
                  <ref
                     target="http://extensions.openoffice.org/en/project/latin-spelling-and-hyphenation-dictionaries">
                     <hi rend="color(#1155cc)underline"> </hi>
                  </ref>
                  <ptr
                     target="http://extensions.openoffice.org/en/project/latin-spelling-and-hyphenation-dictionaries"
                  />
               </p>
               <p>Sabatini, F. 1965. 'Esigenze di realismo e dislocazione morfologica in testi
                  preromanzi', in Rivista di Cultura Classica e Medievale 7, 972–998.</p>
               <p>
                  <hi>Whitaker, W. Latin-English Dictionary Program WORDS (version 1.97FC):</hi>
                  <ref target="http://archives.nd.edu/whitaker/%20words.htm">
                     <hi rend="color(#1155cc)underline"> </hi>
                  </ref>
                  <ref target="http://archives.nd.edu/whitaker/%20words.htm">
                     <hi rend="color(#1155cc)underline">http://archives.nd.edu/whitaker/
                        words.htm</hi>
                  </ref>
               </p>
               <p>Wood C. et al. 2011. 'A Longitudinal Study of Children's Text Messaging and
                  Literacy Development', in British Journal of Psychology, 102(3), 431–442.</p>
            </div>
            <div type="div2" rend="P3">
               <head>
                  <hi>Yves Ouvrard and Philippe Verkerk: </hi>
                  <hi>Collatinus: Lemmatizer and morphological analyzer for Latin texts</hi>
               </head>
               <p>
                  <hi>Collatinus is a free open-source program to lemmatize Latin words.</hi>
                  <hi> </hi>
                  <hi>It has been originally</hi>
               </p>
               <p>
                  <hi>developed by Yves Ouvrard for teaching. It allows to generate a complete
                     lexical aid, with a short translation and the morphological analyses of the
                     forms, for any text which can be given to the students. The program knows the
                     quantity of the vowels, which gives the possibility to scan a text. It also
                     allows to search for a word in the dictionaries, either in a digital form
                     (Lewis &amp; Short, Georges...) or as images (Quicherat, Gaffiot_1934,
                     Calonghi...). The present version is available for any platform (Mac OS X,
                     Windows or Linux) on the website of the Equipex Biblissima.</hi>
                  <hi> </hi>
                  <hi>Hopefully, by the time of the conference, the new version will be available
                     too. It will include the Gaffiot_2016 by Gérard Gréco and his team</hi>
                  <hi> </hi>
                  <hi>and also a probabilistic tagger trained on the texts lemmatized by the
                     LASLA.</hi>
                  <hi> </hi>
                  <hi>As any open-source program, Collatinus can be tuned to meet any particular
                     problem.</hi>
               </p>
               <p>
                  <hi>For any form, Collatinus tries to decompose and analyze it. It then gives all
                     the possible morphological analyses and root-words for the inflected form. To
                     do that, Collatinus has a base of knowledge, which is easily expandable. It
                     consists in a list of root-words (with a short </hi>
                  <hi>translation, mainly in French and in English) and the rules for the flexion of
                     each paradigms. All the files are in plain text and (almost) readable by the
                     user. The automatic treatment of the digital dictionaries (L&amp;S, Georges and
                     Gaffiot 2016) allowed us to expand the lexical base from the original handmade
                     list of 11 000 root-words to a “complete” list of more than 80 000 words.</hi>
                  <hi> </hi>
                  <hi>Adding lemmas with spelling variants (such as medieval spellings, for example)
                     would make it possible to recognize all of their inflected forms as well. When
                     vowel quantities are known for a given word, Collatinus can scan the form and
                     even the entire text. When scanning a text, Collatinus applies the usual rules
                     of lengthening, elision and hiatus. The metrical structure of verses can thus
                     appear immediately. From the quantities, Collatinus can also deduce the
                     position of the accent. For studies in medieval latin, the rhythm of the
                     sentence (or of the clausulae) can be important. Collatinus knows the basic
                     rules for syllabification of latin words, and can learn the etymological
                     exceptions. The examination of the texts lemmatized by the LASLA allows us to
                     retrieve statistical information about the use of the lemmas. This information
                     is now used to order the solutions when a form is ambiguous. To go further, a
                     tag is associated to the analyzed form. This tag is based on the POS of the
                     word and the morphological analysis of the form. Then, for a sentence, we
                     compute the probability for all the possible sequences of tags and we select
                     the most probable one. The probabilities are extracted from the LASLA's corpus
                     and consist simply in the number of occurrences for each succession of three
                     tags. It is thus a probabilistic tagger based on a hidden 2nd order Markov
                     model.</hi>
               </p>
            </div>
            <div type="div2" rend="P3">
               <head>
                  <hi>Neel Smith, Charles Schufreider, Melody Wauke: </hi>
                  <hi>Recovering the history of Iliadic scholia: architecture and initial results
                     from the Homer Multitext project (HMT)</hi>
               </head>
               <p>Scholars of Greek and Latin face particular challenges in developing and analyzing
                  digital text corpora. The morphological richness of the languages requires
                  language-specific pre-processing in order to apply methods such as topic modelling
                  or mapping semantic spaces with embedded word vectors. We must also account for
                  the preservation of our texts on physical artifacts such as manuscripts, since
                  this can provide valuable contextual information for textual analysis. This paper
                  briefly summarizes how we manage this complexity in analyzing texts from the HMT,
                  and presents some initial results reconstructing the transmission of material in
                  the scholia of the oldest complete manuscript of the Iliad.</p>
               <p>We model our texts abstractly as a sequence of semantically meaningful tokens.
                  When HMT editors create archival documents in TEI-conformant XML following
                  project-specific guidelines, a code library ensures that their markup complies
                  with the model. From this point onward, we work not with complex XML, but with
                  structured sets of tokens. Thus we can cleanly separate the tasks of preparing an
                  archival edition using TEI XML, from the tasks of analyzing a text.</p>
               <p>We generate editions targeted at specific types of analysis. The editions are
                  citable at the level of the token, and free of any markup. A pure diplomatic
                  edition, for example, expresses exactly what editors see on the manuscript page; a
                  morphologically normalized edition expresses the same text in modern orthography
                  that can be uniformly parsed.</p>
               <p>We have developed an edition that is optimized for topic modelling. Our work
                  differs from earlier scholarship in systematically analyzing all scholia from the
                  18 books we cover. It is also unique in relating the scholia to the manuscript’s
                  carefully designed layout with five distinct zones for the scholia on each
                  page.</p>
               <p>In the model we built from this edition, we were able to recognize a number of
                  distinct topics, and analyzed their distribution in the visual zones of the page.
                  One example we discuss is focused on readings of the seminal editor Aristarchus.
                  59% of the scholia most strongly correlated with this topic occur in the
                  intermarginal zone between the main scholia and the Iliad text, yet the
                  intermarginal zone comprises only 15% of the scholia overall. The skewed
                  distribution means that the pattern of language associated with Aristarchus occurs
                  much more frequently in the intermarginal zone than in any other. The distribution
                  of material in each of the zones is clearly not uniform.</p>
               <p>The Aristarchan topic is not unique to the intermarginal zone, however, and in
                  some cases main and intermarginal scholia even repeat the same point about
                  Aristarchus. The scribe’s organization of the page into zones therefore does not
                  simply define zones by subject matter. We argue that the zones reflect distinct
                  sources of material the scribe used. The frequency of the Aristarchan topic in the
                  intermarginal scholia could be annotations of a source that was routinely checked
                  against the longer discussion of the main scholia.</p>
            </div>
            <div type="div2" rend="P3">
               <head>
                  <hi>Francesco Mambrini: </hi>
                  <hi>Idiolects of the Heroes? Towards a treebank-based approach to the language of
                     the Sophoclean characters</hi>
               </head>
               <p>
                  <hi>The critical debate about the concept of “character” in literary works as well
                     as about the relation between character and diction, is as old as the </hi>
                  <hi>Poetics</hi>
                  <hi> </hi>
                  <hi>of Aristotle. In a seminal work, J. Burrows</hi>
                  <hi>[1]</hi>
                  <hi> </hi>
                  <hi>has shown how a computer-assisted analysis of the distribution of frequent
                     words brings to light a series of distinctive linguistic patterns that
                     contribute to the definition of the characters in the novels of Jane Austen.
                     Annotated corpora can potentially increase the productivity of this approach,
                     as they add further layers of abstraction (morphology and syntax) where the
                     linguistic differentiation among characters can be observed.</hi>
               </p>
               <p>
                  <hi>In this presentation, we address the question of how the language of the
                     Sophoclean characters can be studied with the help of the annotation encoded in
                     the Ancient Greek Dependency Treebank (AGDT)</hi>
                  <hi>[2]</hi>
                  <hi>. This corpus, published by the Perseus Project, include five of the seven
                     complete extant tragedies of Sophocles, with the exclusion of </hi>
                  <hi>Philoctetes</hi>
                  <hi> </hi>
                  <hi>and </hi>
                  <hi>Oedipus Coloneus</hi>
                  <hi>. The AGDT allows us to compare the distribution of lexicon, but also of
                     morphological features (parts of speech, cases, verbal moods) and frequent
                     syntactic constructions among the different </hi>
                  <hi>personae</hi>
                  <hi> </hi>
                  <hi>in the plays. With the help of the observed distributions, we intend to test
                     which of the aforementioned morphosyntactic factors, if any, contributes to the
                     differentiation of the speeches of the characters and to provide new answers to
                     fundamental questions. Is it possible </hi>
                  <hi>to speak of “idiolects” of the Sophoclean characters? Do the language of the
                     characters that perform similar roles in the dramas cluster together (as they
                     appear to do in Austen’s novels, according to Burrows)? Or is the distributions
                     of lexical and morphosyntactic features more affected by other formal
                     constraints, like e.g. the metrical context or the opposition between dialogue
                     and lyrics.</hi>
               </p>
               <p>However, this research program entails a series of practical and methodological
                  problems, on which we intend to focus our presentation. To begin with, as the
                  information about the speakers is not recorded in the treebank, the attribution of
                  each sentence to the characters is already a non trivial challenge at the scale of
                  thousands of sentences. For dramatic texts, whose digital editions in the Perseus
                  Digital Library encode information about the speakers, the problem can be solved
                  readily. On the other hand, for other texts from the AGDT that could serve as
                  basis for comparison with Sophocles (like the Homeric poems, or the direct
                  speeches in Herodotus or Thucydides), the automatic attribution of the sentences
                  requires the use of more complex techniques of text analysis. The practical
                  challenge bears also on the important question of corpus representativity. What
                  texts should we use to compare the Sophoclean data to? How does the interpretation
                  of the data change if we change the reference corpus?</p>
               <p>
                  <hi>[1]</hi>
                  <hi> </hi>
                  <hi>J.F. Burrows. Computation into Criticism. A Study of Jane Austen's Novels.
                     Oxford: Clarendon Press, 1987.</hi>
               </p>
               <p>
                  <hi>[2]</hi>
                  <ref target="https://perseusdl.github.io/treebank_data/">
                     <hi rend="color(#1155cc)underline"> </hi>
                  </ref>
                  <ptr target="https://perseusdl.github.io/treebank_data/"/>
               </p>
            </div>
            <div type="div2" rend="P5">
               <head>Lou Burnard: What is TEI conformance, and why should you care?</head>
               <p>For more than three decades, the recommendations of the Text Encoding Initiative
                  (TEI) have been a defining feature of the methodological framework of the Digital
                  Humanities, despite recurrent concerns that the system they define is at the same
                  time both too rigorous for the manifold variability of humanistic text, and not
                  precise enough to guarantee interoperability of resources defined using it. In
                  this talk I propose to explore the notion of conformance proposed by the
                  Guidelines, which seems to operate at both a technical syntactic level, and a less
                  easily verifiable semantic level. I shall suggest that one of the more curious
                  features of the Guidelines is their desire to have (as the French say) both the
                  butter and the money for the butter, and that maybe their continued relevance is
                  in no small part due to this flexibility and adaptability.</p>
            </div>
            <div type="div2" rend="P3">
               <head>
                  <hi>Charlotte Schubert: </hi>
                  <hi>Editing and Un-editing in Digital Classics</hi>
               </head>
               <p>
                  <hi>In my presentation I propose methodological considerations for the adaption of
                     methods from text mining in an interplay of application of algorithm-based
                     methods and epistemic systematization, based on the theory of intertextuality
                     with its concept of author and context. I describe this based on some examples
                     based on the tools of the portal eAQUA (the quotations in Plutarch’s De
                     malignitate Herodoti, a verbatim citation of Anacharis in Sextus Empiricus and
                     a </hi>
                  <hi>new, previously unknown semantic connection between Athens and nomadism
                     referring to a paraphrase from the Atthis of Philochorus, eAQUA:</hi>
                  <ref target="http://www.eaqua.net/">
                     <hi rend="color(#1155cc)underline"> </hi>
                  </ref>
                  <ref target="http://www.eaqua.net/">
                     <hi rend="color(#1155cc)underline">www.eaqua.net</hi>
                  </ref>
                  <hi>). Further, I will give a short outlook towards our new project “DIGITAL
                     PLATO”, funded by the Volkswagenstiftung. In this project we will compile
                     testimonies, citations and paraphrases of Plato’s work in the form of a digital
                     thesaurus to reveal the aftermath and reception of Plato’s work in ancient
                     Greek literature.</hi>
               </p>
            </div>
            <div type="div2" rend="P3">
               <head>
                  <hi>Gregory Crane: </hi>
                  <hi>Greek and Latin in an Age of massive collections and global philology</hi>
               </head>
               <p>What are the implications of scale for Greek and Latin? Scale includes not only
                  new,  more expressive forms of publishing particular scholarly judgments (such as
                  morpho-syntactic analyses) and the challenge of working with billions of words in
                  Greek and Latin but also engaging with a global public and scholarly community,
                  one where centers of gravity from outside of Europe and the Americas are taking
                  shape.</p>
            </div>
            <div type="div2" rend="P3">
               <head>
                  <hi>Rodney Ast: </hi>
                  <hi>Can the digital humanities make us better humanists? A case study in
                     Papyrology</hi>
               </head>
               <p>Individual digital projects affect their target disciplines in varying ways: some
                  support traditional scholarly methods while others transform them. Although we
                  often recognize, at least intuitively, the importance of individual initiatives to
                  our disciplines, we seldom reflect on the nature and extent of their impact, on
                  how exactly digital tools improve and even redefine how we work. This talk
                  attempts some of this reflection in the context of papyrological and allied
                  digital initiatives. Among questions to be asked are: How do the Digital
                  Humanities make us better humanists? What basic scholarly methodologies and
                  behaviors are supported by our digital tools? How and why are some tools more
                  effective than others? The aim is to consider how digital papyrology has over the
                  past several decades served basic research behaviors or "scholarly primitives," to
                  borrow a term from John Unsworth's seminal 2000 article, and how it might serve
                  them better in the future.</p>
            </div>
            <div type="div2" rend="P3">
               <head>
                  <hi>Neven Jovanović: </hi>
                  <hi>Atomic philology and parallel philology – some implications of the CITE
                     architecture</hi>
               </head>
               <p>The CITE architecture, developed from 2006 by Neel Smith and Christopher
                  Blackwell, is an editorial concept with a potential to transform digital
                  philology, primarily because it is able to solve two difficult problems - it deals
                  with overlapping textual hierarchies, and it connects annotations with the
                  relevant textual segments. The CITE framework also impels us to think more
                  precisely about what we are doing as philologists. We realize that our work begins
                  at the "atomic" level of a text, and that interpretative processes happen "in
                  parallel".</p>
               <p>CITE architecture gives the editor full control over (and responsibility for) the
                  level of segmentation of the text. Each segment, regardless of what it consists
                  of, is marked by a hierarchically structured Uniform resource identifier. The
                  problem of overlapping hierarchies - for example, the standard Stephanus page
                  divisions which may clash with a logical division of Plato's text into sentences
                  and paragraphs - is thus solved by proposing a compound digital edition. Such
                  edition must not be limited to one text containing all of the editor's theories
                  about it; it may comprise multiple "analytical exemplars" - in the case of Plato,
                  one such exemplar may be divided into Stephanus pages, the other into paragraphs
                  and sentences, the third into individual words, the fourth may consist only of
                  "key words" expressing key philosophical contexts. Because each segment in each of
                  these parallel, single-function exemplars has a unique URI, it is possible (and
                  necessary) to construct a concordance table, saying that this passage in one
                  exemplar corresponds to that one in the other exemplar. The architecture also
                  extends the concept of unique URIs to the annotations - each note bears a URI
                  pointing to the relevant textual segment (in the relevant exemplar).</p>
               <p>We have tested the CITE architecture by applying it to texts of Croatian Latin,
                  most of them early modern works (the earliest text is from the 976, the latest
                  from 1984) written by Croatian and other authors connected with people and regions
                  of today's Croatia. We have also used the CITE annotations to describe lexical and
                  grammatical features of selected words (place names and place references), but
                  also to describe realia - actual (or sometimes imaginary) places and periods
                  signified by these words.</p>
               <p>The experience leads me to propose a new vision of philology - the discipline as
                  an ability to manipulate texts at their most basic, "atomic" level, with such
                  precision that interpretations arise from studying different configurations of
                  numerous simple "philological atoms". Once we are able to follow a certain
                  combination - for example, a real place in an imaginary time, denoted by different
                  words in different grammatical forms - through a large number of texts, the
                  interpretation happens almost by itself.</p>
            </div>
            <div type="div2" rend="P3">
               <head>
                  <hi>James Cowey: </hi>
                  <hi>Challenges and opportunities for the discipline of Papyrology in the age of
                     digital editions</hi>
               </head>
               <p>In April of 2010 papyri.info was inaugurated. It is a fully TEI conformant XML
                  tagged set of files for texts in Greek, Latin and Coptic. For many papyrologists
                  this new platform was and still is a welcome way of being able to search for
                  information about phraseology, word occurrence or about date and content, to
                  mention just a few possibilities.</p>
               <p>More importantly this platform makes it possible to enter newly published editions
                  to the system. Such editions may be newly keyed in existing paper editions made
                  digital. It is, however, possible to use the system to produce born digital
                  editions. All such work is peer reviewed and then published directly to the web.
                  Once entered they appear within hours and belong to the new search mass.</p>
               <p>Through community based voluntary work the papyrologists have been able to keep
                  abreast of the entry of new publications and have them at their finger tips
                  immediately.</p>
               <p>This has changed several perceptions of what the texts are. We are still very much
                  at a cross roads between the traditional way of papyrological scholarly editing
                  and the opportunities that the digital age offers us to in terms of online
                  editing.</p>
               <p>By looking at specific cases I will examine the consequences for Papyrology as a
                  subject and how research methods are changing. There are many advantages, but also
                  disadvantages that must be borne in mind. Using marked up text to carry out
                  research ends up telling us about the consistency of our mark up. When texts are
                  marked up in TEI conformant XML then we can formulate very precise ways of
                  searching. Our returns are only as good as the mark up. There will be cases of
                  false entry or texts which has been entered as straight text, although one would
                  expect it to have been entered with the relevant mark up. This gives rise to
                  reflections on what this means for the discipline and what it tells us about how
                  we have proceeded.</p>
               <p>There is also the very important aspect of texts which are constantly in flux. Old
                  style paper editions of text on papyri, ostraca or other materials fixed the
                  deciphered text at any one specific point in time. Scholars would make reference
                  to this edition. Nowadays when improvements are made to individual parts of texts,
                  the whole text including the improved section is now what is used. Of course this
                  version of the text may change tomorrow. This obviously offers challenges and at
                  times unnerves many scholars.</p>
               <p>These points will be discussed in the paper in the hope that this will stimulate
                  further discussion and offer topics for further consideration.</p>
            </div>
            <div type="div2" rend="P3">
               <head>
                  <hi>Nicola Reggiani: </hi>
                  <hi>The digital edition of ancient sources as a further step in the textual
                     transmission</hi>
               </head>
               <p>Critical editions of ancient sources are usually considered as the final outcome
                  of a philological process of reconstruction aimed at reproducing the original text
                  as most exactly as possible. Thus they are the fixed representation of a scholar's
                  more or less trustable opinion on the text. Ancient studies today are
                  characterised by a more and more widespread presence of the digital technologies,
                  which allow us to redefine and reshape the concept itself of critical edition,
                  more focused on the actual testimonies of the textual tradition and on their
                  peculiar features. Multitextual editions, for example, tend to replace the
                  traditional structure of text and apparatus criticus with a more dynamic network
                  of multiple editions interconnected to each other; markup tagging and linguistic
                  annotation, on the other hand, add deeper and deeper levels of in-text
                  information. It can be argued, therefore, that a digital (critical) edition can
                  develop into something completely different than the “old-fashioned” print
                  critical edition: namely a further step in the fluid textual transmission of
                  ancient sources.</p>
            </div>
         </div>
      </body>
   </text>
</TEI>
