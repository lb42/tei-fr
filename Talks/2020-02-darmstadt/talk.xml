<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:svg="http://www.w3.org/2000/svg"
  xmlns:math="http://www.w3.org/1998/Math/MathML" xmlns="http://www.tei-c.org/ns/1.0">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title>Modelling meaning: the role of data in the humanities </title>
        <title type="sub">He who sees the Infinite in all things sees God. He who sees the Ratio
          only, sees himself only</title>
        <author>Lou Burnard</author>

      </titleStmt>
      <publicationStmt>
        <p>Unpublished draft for presentation at Darmstadt</p>
      </publicationStmt>
      <sourceDesc>
        <p>Various old talks and TEI-L</p>
      </sourceDesc>
    </fileDesc>
    <revisionDesc>
      <change when="2020-01-21">Revising draft for Darmstadt</change>
      <change when="2019-04-01">First draft for Paderborn</change>
    </revisionDesc>
  </teiHeader>
  <!--He who sees the Infinite in all things sees God. He who sees the Ratio only, sees himself only".-->

  <text>
    <front>
      <div type="slide">
        <head>Abstract</head>
        <p>Epistemological arguments about knowledge, justification, and the rationality of belief
          almost always have recourse to considerations of something called “data”: we say “this is
          data driven” in a positive sense, or “show me the data” in a negative one. We talk
          approvingly of data-driven learning, and of evidence-based medicine. We systematically
          claim to prefer the empirical to the “armchair" variety of linguist, ostensibly on the
          grounds of objectivity. But as Stanley Fish pointed out a very long time ago, no matter
          how objective our data-gathering methods, there comes a point at which the data has to be
          interpreted. And let us not also forget that the data we choose to analyse in the
          humanities is itself socially and historically determined: vast as they are, the digital
          collections of Hathitrust or Googlebooks or Gallica represent in their composition
          specific accidents of curatorial history and the vagaries of changing cultural priorities
          over time more than they represent the actual facts of book production or consumption,
          even though -- paradoxically -- we may have no better way of determining those facts. In
          this respect, the proponents of "culturonomics" have perhaps something to learn from
          discourse analysis and the emphasis it places on the social and historical context within
          which speech data – whether written or spoken – occurs, is captured and curated. Context,
          the framing of an observation, the metadata around the data, is crucial no matter how big
          your big data may be, as Katherine Bode amongst others has noted. In addressing the themes
          proposed for this seminar, I will first try to place the issues surrounding data and data
          modelling in a historical context, arguing that while it was once convenient to consider
          text as a special kind of data, today in a time of data-excess it is perhaps more
          appropriate to consider data as a special kind of text, with the same affordances and the
          same rugosities whatever form it may take.</p>


      </div>
    </front>
    <body>
      <div type="slide">
        <head>The brief</head>
        <list>
          <item> What is the status of data in the humanities and social science research of our
            time? </item>
          <item>How has data, its encoding and markup, changed the way discourse studies perceive
            their subject? </item>
          <item>Does data driven discourse research change its role in society? </item>
          <item>How does data impact scientific understanding and what is its role in
            epistemological processes? </item>
          <item>What is the role of data in field work between experience, encounter, and
            interaction? </item>
          <item>What ethical implications arise in handling research data? </item>
          <item>In what direction will (and should) discourse research develop and what role will
            data play in it? </item>
        </list>
        <p>(Sorry, I won't answer all these questions)</p>
      </div>
      <div type="slide">
        <head>The status of data in current SHS research</head>
        <list>
          <item>Data is omnipresent, but not entirely omniscient</item>
          <item>Some disciplines are almost entirely data-dependent (e.g. corpus linguistics,
            stylometrics)</item>
          <item>In others (e.g. literary studies) data-dependence remains controversial</item>
          <item>The massive expansion of data availability has lead to a recognition of the
            centrality of data-modelling</item>
          <item>... but there are two kinds of modelling : <list>
              <item>in the traditional humanities, a model is a means of abstraction and a set of
                categories: a <emph>reductive</emph> process</item>
              <item>in the social sciences, a model is a tool for prediction and generalisation : an
                  <emph>analytic</emph> process</item>
            </list></item>
        </list>
        <p rend="box">(see e.g. Flanders and Jannidis 2019)</p>
      </div>
      <div type="slide">
        <head>Encoding data for discourse studies</head>
        <list>
          <item>(CAVEAT: I am not a discourse studies person)</item>
          <item>discourse components cross multiple levels</item>
        </list>
        <p>Which level/s of description do we favour? </p>
        <figure>
          <graphic url="media/oral-annot.png"/>
        </figure>
      </div>

      <div type="slide">
        <head>Multiple levels can coexist in XML</head>
        <figure>
          <graphic url="media/steven-xml.png"/>
        </figure>

      </div>

      <div type="slide">
        <head>But in practice, linguists seem to prefer fairly simple -- reductive -- data
          categorisations </head>
        <p>A choice must be made...</p>
        <figure>
          <graphic url="media/spokenBits.png"/>
        </figure>
        <cb/>
        <figure>
          <graphic url="media/dartExample.png" height="89%"/>
        </figure>
      </div>
      <div type="slide">
        <head>Is there any such thing as a "pure" transcription?</head>

        <p>A language corpus consists of samples of authentic language productions ... <list>
            <item>selected according to explicit principles, for specific goals</item>
            <item>represented in a digital form</item>
            <item>generally enriched with metadata and annotation beyond "pure" transcription
            </item>
          </list></p>

        <p rend="box">Can there be any re-presentation without interpretation? </p>

      </div>

      <div type="slide">
        <head>Annotation: necessary evil or fundamental ?</head>

        <p rend="box"><q>Annotation ... is anathema to corpus-driven linguists.</q> (Aarts, 2002) </p>

        <p rend="box"><q>The interspersing of tags in a language text is a perilous activity,
            because the text thereby loses integrity.</q> (Sinclair, 2004)</p>

        <p rend="box"><q> … the categories used to annotate a corpus are typically determined before
            any corpus analysis is carried out, which in turn tends to limit ... the kind of
            question that usually is asked.</q> (Hunston, 2002)</p>
        <list>
          <item>transcription represents the transcriber's understanding of the source</item>
          <item>encoding/annotation represents the annotator's intuitions about it, in a codified
            form</item>
          <item>how are these different?</item>
        </list>
        <p rend="box">
          <q>... all encoding interprets, all encoding mediates. There is no 'pure' reading
            experience to sully. We don't carry messages, we reproduce them –– a very different kind
            of involvement. We are not neutral; by encoding a written text we become part of the
            communicative act it represents. </q> (Caton 2000) </p>
      </div>

      <!--
      <div type="slide">
        <head>Principles of annotation (Leech, 2005) </head>

        <p rend="box">annotation is ... the practice of adding interpretative linguistic information
          to a corpus </p>
        <list>
          <item>the annotation should be separable from the text</item>
          <item> multiple annotations may co-exist within the text</item>
          <item> annotation should be <list>
              <item> self-documenting</item>
              <item> explicit</item>
              <item> reproducible</item>
              <item> formally verifiable</item>
            </list>
          </item>
        </list>
        <p rend="box">This applies also to the metadata associated with the corpus</p>
      </div>
      
      <div type="slide">
        <head>Data in field work</head>
        <!-\- experience/encounter/interaction-\->
      </div>
      <div type="slide">
        <head>Ethical considerations</head>
        <list>
          <item>privacy and interconnexions</item>
        </list>
      </div>
      <div type="slide">
        <head>Future directions for discourse studies</head>
        <list>
          <item>(CAVEAT: I am not a discourse studies person)</item>
        </list>
      </div>
-->


      <div type="slide">
        <head>A naive realist's manifesto</head>
        <figure>
          <graphic url="media/model.png"/>
        </figure>
        <p>How do we keep the virtuous hermeneutic circle turning? </p>
        <p rend="box">Modelling matters</p>


      </div>
      <div type="slide">
        <head>How did we get here from there?</head>
        <p>Let's (briefly) go back to the unfamiliar world of the mid-1980s... <list rend="pause">
            <item>the world wide web did not exist</item>
            <item>the tunnel beneath the English Channel was still being built</item>
            <item>a state called the Soviet Union had just launched a space station called
              Mir</item>
            <item>serious computing was done on mainframes </item>
            <item>the world was managing quite nicely without the DVD, the mobile phone, cable tv,
              or Microsoft Word</item>
          </list></p>
      </div>
      <div type="slide">
        <head>...but also a familiar one</head>
        <list>
          <item>corpus linguistics and <q>artificial intelligence</q> had created a demand for large
            scale textual resources in academia and beyond</item>
          <item>advances in text processing were beginning to affect lexicography and document
            management systems (e.g. TeX, Scribe, (S)GML ...)</item>
          <item>the Internet existed for academics and for the military; theories about how to use
            it <q>hypertextually</q> abounded </item>
          <item>books, articles, and even courses in something called "Computing in the Humanities"
            were beginning to appear</item>
        </list>
      </div>
      <div type="slide">
        <head>Modelling the data vs modelling the text</head>
        <p>By the end of the 1970s, methods variously called <q>data modelling</q>, <q>conceptual
            analysis</q>, <q>database design</q> vel sim. had become common practice.</p>
        <list>
          <item>remember: a centralised mainframe world dominated by IBM</item>
          <item>spread of office automation and consequent data integration</item>
          <item>ANSI SPARC three level model</item>
        </list>
        <figure>
          <graphic url="media/ansi-sparc.png"/>
        </figure>
      </div>
      <div type="slide">
        <head>An inherently reductive process</head>
        <figure>
          <graphic url="media/ansi-sparc-2.png"/>
        </figure>
        <p rend="box">how applicable are such methods to the complexity of humanities data
          sources?</p>
      </div>


      <div type="slide" n="18">
        <head>The 1980s were a period of technological enthusiasm</head>
        <list>
          <item>Digital methods and digital resources, despite their perceived strangeness were
            increasingly evident in the Humanities </item>
          <item>There was some public funding of infrastructural activities, both at national and
            European levels: in the UK, for example, the <title>Computers in Teaching
              Initiative</title> and the <title>Arts and Humanities Data Service</title></item>
          <item>Something radically new, or just an update ? </item>
          <item> Humanities Computing (aka Digital Humanities) gets a foothold, by establishing
            courses </item>
          <item>Thaller (in 1989) challenges advocates of <q>Humanities Computing</q> to define its
            underlying theory </item>
          <item>Unsworth and others (by 2002) start using the phrase ”scholarly primitives” to
            characterise a core set of procedures sustaining something called <q>Digital
              Humanities</q>
          </item>
        </list>
        <figure>
          <graphic url="media/slide20.png" height="30%"/>
        </figure>
      </div>
      <div type="slide" n="18">
        <head>Where did these digital methods originally thrive?</head>
        <list>
          <item>corpus linguistics</item>
          <item>authorship and stylometry</item>
          <item>historical data</item>
        </list>

      </div>
      <div type="slide" n="21">
        <head>Corpus Linguistics : searching for meaning </head>
        <!--
  1959: Survey of English Usage (Quirk et al)
  1965: Computational Analysis of Present-Day American English (Brown Corpus)
  
  -->
        <p>How do we identify the components of a discourse which give it meaning ?</p>

        <list>
          <item>meaning is usage: <q>Die Bedeutung eines Wortes ist sein Gebrauch in der Sprache</q>
            (Wittgenstein, 1953)</item>
          <item>meaning is collocation <q>You shall know a word by the company it keeps</q> (Firth,
            1957) </item>
        </list>
        <!--  <list>
          <item>Cruden (1730) Concordance to the Holy Scriptures </item>
        </list>-->

        <p rend="box">the text is the data</p>

      </div>

      <div type="slide" n="18">
        <head>Stylometrickery</head>
        <list>
          <item>text as a bag of words</item>
          <item>statistical analysis of word frequencies to determine authorship and quantify
            style</item>
          <item>from T C Mendenhall (1887) to J F Burrows (1987)</item>
          <item>... now reborn as distant reading, culturonomics </item>
        </list>
        <p rend="box">the text is the data</p>
      </div>

      <div type="slide" n="21">
        <head>The re-invention of <foreign>quellenkritik</foreign></head>
        <p rend="box"><q>History that is not quantifiable cannot claim to be scientific</q> (Le Roy
          Ladurie 1972)</p>
        <list>
          <item>In the UK, a series of <hi>History and Computing</hi> (1986-1990) conferences showed
            historians already using commercial DBMS, data analysis tools developed for survey
            analysis, "personal database systems" ... </item>
          <item>In France, J-P Genet and others influenced by the <hi>Annales</hi> school proposed a
            programme of digitization for historical sources records </item>
          <item>Further pursued by Manfred Thaller with the program <emph>kleio</emph> (1982) -- a
            tool for transcribing and analysing (extracts from) historical sources, which included
            annotation of their content/significance</item>
        </list>
        <p rend="box">the data is extracted from the text</p>
      </div>

      <!--  <div type="slide" n="22">
        <head>Theorizing Humanities Computing</head>
        <list>
          <item>What <hi>are</hi> the underlying principles of the tools used in Humanities
            Computing (then) or Digital Humanities (now)?</item>
          <item>Unsworth and others eventually (by 2002) start using the phrase ”scholarly
            primitives” to characterise a core set of procedures e.g. <list>
              <item><hi>searching</hi> on the basis of externally-defined features </item>
              <item><hi>analysis</hi> in terms of internally-defined features</item>
              <item><hi>association</hi> according to shared readings </item>
            </list></item>
        </list>
      <figure>
          <graphic url="media/scholPrim.png" width="40%"/>
          <head>(Hughes 2012)</head>
        </figure>

        <p rend="box">Isn't the <hi>modelling of textual data</hi> at the heart of all these? </p>

      </div>
-->

      <div type="slide">
        <head>How should we model textual data?</head>
        <figure>
          <graphic url="media/txttrin.png" height="70%"/>
          <head>The Textual Trinity (Burnard 1987)</head>
        </figure>
        <quote>In interpreting text, the trained human brain operates quite successfully on three
          distinct levels; not surprisingly, three distinct types of computer software have evolved
          to mimic these capabilities.</quote>
      </div>

      <div type="slide">
        <head>Text is little boxes</head>
        <figure>
          <!--<quote>The main idea is to consider the process as an operation on two-dimensional "boxes";
          roughly speaking, the input is a long string of characters culled from a variety of fonts,
          where each character may be thought of as occupying a small rectangular box, and the
          output is obtained by gluing these boxes together either horizontally or vertically with
          various conventions about centering and justification, finally arriving at big rectangular
          boxes which are the desired pages. </quote>-->
          <graphic url="media/texQuote.png" width="80%"/>
          <head>(Preliminary description of TEX: D Knuth, May 13, 1977)</head>
        </figure>
        <list>
          <item><hi>TeX</hi> was developed by Donald Knuth, a Stanford mathematician, to produce
            high quality typeset output from annotated text </item>
          <item>Knuth also developed the associated idea of <hi>literate programming</hi>: that
            software and its documentation should be written and maintained as an integrated
            whole</item>
          <item>TeX is still widely used, particularly in the academic community: it is open source
            and there are several implementations</item>
        </list>
      </div>
      <div type="slide">
        <head>No, text is data</head>
        <figure>
          <graphic url="media/germanDict.jpg" height="90%"/>
        </figure>
      </div>

      <div type="slide">
        <head>Database orthodoxy</head>
        <list>
          <item>identify important entities which exist in the real world and the relationships
            amongst them</item>
          <item>formally define a conceptual model of that universe of discourse</item>
          <item>map the conceptual model to a storage model (network, relational,
            whatever...)</item>
        </list>
        <p rend="box">But what are the "important entities" we might wish to identify in a textual
          resource? </p>
      </div>

      <div type="slide">
        <head>Assize court records, for example</head>
        <figure>
          <graphic url="media/1671assizes.jpg" width="80%"/>
        </figure>
        <cb/>

        <figure>
          <graphic url="media/recogModel.png" width="90%"/>
          <head>(<title>An application of CODASYL techniques to research in the humanities</title>,
            1980) </head>
        </figure>


      </div>
      <!--

      <div type="slide">
        <head>Scribe</head>
        <p><hi>Scribe</hi> developed by Brian K Reid at Carnegie Mellon in the 1980s, was one of the
          earliest successful document production systems to separate content and format, and to use
          a formal document specification language. Its commercial exploitation was short-lived, but
          its ideas were very influential. </p>
        <figure>
          <graphic url="media/scribe-1.png"/>
        </figure>
        <figure>
          <graphic url="media/scribe-2.png"/>
        </figure>
    -->
      <!--  Reid, Brian K. "The Scribe Document Specification Language and its Compiler.".
          Pages 59-62 (with 10 references) in International Conference on Research and Trends in Document Preparation Systems. 
          Abstracts of the Presented Papers. Conference on Research and Trends in Document Preparation Systems, Lausanne, Switzerland, February 27-28, 1981. 
          Supported and organized by the [Swiss] Conseil des Ecoles Polytechniques Fédérales. J. D. Nicoud, Program Chair. Lausanne/Zürich: Swiss Federal Institutes of Technology, 1981. v + 130 pages. Author affiliation: Computer Systems Laboratory, Stanford University, Stanford, CA USA 94305.
-->

      <!--    </div>
  -->
      <!-- <div type="slide">
        <head>(S)GML</head>
        <!-\- <quote>Historically, electronic manuscripts contained control codes or macros that caused the
          document to be formatted in a particular way ('specific coding'). In contrast, generic
          coding, which began in the late 1960s, uses descriptive tags (for example, 'heading',
          rather than 'format-17') ...</quote>
   -\->
        <figure>
          <graphic url="media/sgmlQuote.png" width="70%"/>
          <head>(A Brief History of the Development of SGML (C)1990 SGML Users' Group )</head>
        </figure>
        <p>Charles Goldfarb and others developed a "Generalized Markup Language" for IBM, which
          subsequently became an ISO standard (ISO 8879: 1986) </p>
        <p>SGML was designed to enable the sharing and long term preservation of machine-readable
          documents for use in large scale projects in government, the law, and industry. the
          military, and other industrial-scale publishing industries. </p>
        <p rend="box">SGML is the ancestor of HTML and of XML ... it defined for a whole generation
          a new way of thinking about <hi>what text really is</hi></p>
      </div>
-->
      <!-- <div type="slide">
        <head>Motivations for SGML</head>
        <list>
          <item>an enormous increase in the quantity of technical documentation : the aircraft
            carrier story</item>
          <item>an enormous increase in its complexity : the Gare de Lyon story </item>
          <item>a proliferation of mutually incompatible document formats </item>
          <item>an almost evangelical desire for centrally-defined standards</item>
          <item>a mainframe-based, not yet distributed, world in transition </item>
        </list>
        <p rend="box">The method: apply database modelling techniques to textual documents! </p>
      </div>-->
      <div type="slide">
        <head>What is a text (really)?</head>
        <list>
          <item>content: the components (words, images etc). which make up a document </item>
          <item>structure: the organization and inter-relationship of those components </item>
          <item>presentation: how a document looks and what processes are applied to it </item>
          <item>context: how the document was produced, circulated, processed, and understood</item>
          <item>.. and possibly many other readings</item>
        </list>
        <cb/>
        <p>For example: </p>
        <figure>
          <graphic url="media/19790809_002v.jpg" height="60%"/>
        </figure>

      </div>


      <div type="slide">
        <head>Separating content, structure, presentation, and context means : </head>
        <list>
          <item> the content can be re-used </item>
          <item> the structure can be formally validated </item>
          <item> the presentation can be customized for <list>
              <item>different media </item>
              <item>different audiences</item>
            </list></item>
          <item>the context can be analysed</item>
          <item> in short, the information can be uncoupled from its processing</item>
        </list>
        <p> This is not a new idea! But is it a good one? </p>
      </div>

      <div type="slide">
        <head>Some ambitious claims ensued </head>
        <figure>
          <graphic url="media/xml-slide.png"/>
          <head>(Presentation for Oxford IT Support Staff Conference, 1994)</head>
        </figure>
        <!-- not the
        real date -->
      </div>
      <!--

      <div rend="slide">
        <head>The role of markup today</head>
        <p>The humanities are all about <emph>text</emph>
          <list>
            <item>(non-digital) books, manuscripts, archival papers...</item>
            <item> ... as well as other -\- increasingly digital -\- cultural manifestations such as
              sounds, images, blogs, tweets ... </item>
          </list></p>
        <p>The digital humanities are all about digital technologies and techniques for manipulating
          such manifestations in an integrated and innovative way. </p>
        <p rend="box"><hi>Markup</hi> (aka encoding or tagging) is the key technology behind such
          integration.</p>
      </div>
-->
      <div type="slide">
        <head>A digital text may be ... </head>
        <p rend="box">a <q>substitute</q> (surrogate) simply representing the appearance of an
          existing document</p>
        <figure>
          <graphic url="media/graves-2.png" height="80%"/>
        </figure>
      </div>
      <div type="slide">
        <head> ... or it may be</head>
        <p rend="box">a representation of its linguistic content and structure, with additional
          annotations about its meaning and context.</p>
        <figure>
          <graphic url="media/graves-1.png" height="80%"/>
        </figure>
      </div>
      <div type="slide">
        <head>Functions of encoding</head>
        <list>
          <item>It makes explicit to a processor <emph>how</emph> something should be
            processed.</item>
          <item>In the past, <soCalled>markup</soCalled> was what told a typesetter how to deal with
            a manuscript</item>
          <item>Nowadays, it is what tells a computer program how to deal with a stream of textual
            data.</item>
        </list>
        <p rend="box">... and thus expresses the encoder's view of what <hi>matters</hi> in this
          document, determining how it can subsequently be analysed.</p>
      </div>


      <div type="slide">
        <head>Which textual data matters ?</head>
        <list rend="pause">
          <item>the shape of the letters and their layout?</item>
          <item>the presumed creator of the writing?</item>
          <item>the (presumed) intentions of the creator? </item>
          <item>the stories we read into the writing? </item>
        </list>

        <p rend="box">A <soCalled>document</soCalled> is something that exists in the world, which
          we can <term>digitize</term>.</p>
        <p rend="box">A <soCalled>text</soCalled> is an abstraction, created by or for a community
          of readers, which we can <term>encode</term>.</p>
      </div>
      <div type="slide">
        <head>The document as <q>Text-Bearing Object</q> (TBO)</head>
        <p rend="box"><hi rend="italic">Materia appetit formam ut virum foemina</hi></p>
        <list>
          <item>Traditionally, we distinguish form and content</item>
          <item>In the same way, we might think of an inscription or a manuscript or even a
            transcribed recording as the bearer or container or form instantiating an abstract
            notion -- a text</item>
          <item>but quite a lot of the text is actually all in our head</item>
        </list>
        <p rend="box">And don't forget ... digital texts are also TBOs!</p>
      </div>
      <div type="slide">
        <head>Markup is a scholarly activity</head>
        <list>
          <item>The application of markup to a document is an intellectual activity</item>
          <item>Deciding exactly what markup to apply and why is much the same as editing a text </item>
          <item>Markup is rarely neutral, objective, or deterministic : interpretation is
            needed</item>
          <item>Because it obliges us to confront difficult ontological questions, markup can be
            considered a research activity in itself</item>
          <item>Good textual encoding is never as easy or quick as people would believe -- do things
            better, not necessarily quicker</item>
          <item>The markup scheme used for a project should result from a detailed analysis of the
            properties of the objects the project aims to use or create and of their
            historical/social context </item>
        </list>
        <p rend="box">... though considerations of scale may have an effect ... </p>
      </div>
      <div type="slide">
        <head>Because ... </head>
        <p rend="box">Good markup (like good scholarship) is expensive</p>
        <figure>
          <head>Big data vs. curated data</head>
          <graphic url="media/dataCartoon.png"/>
        </figure>

      </div>
      <div type="slide">
        <head>Choices (1) </head>
        <p>Consider this kind of object: </p>

        <figure>
          <graphic url="media/beowulf-ms.png" height="80%"/>
          <head>BL Ms Cotton Vitelius A xv, fol. 129r</head>
        </figure>
      </div>

      <div type="slide">
        <head>Some typical varieties of curated markup</head>
        <p><egXML xmlns="http://www.tei-c.org/ns/Examples">
            <hi rend="dropcap">H</hi><g ref="#wynn">W</g>ÆT WE GARDE
            <lb/>na in gear-dagum þeod-cyninga
            <lb/>þrym gefrunon, hu ða æþelingas
            <lb/>ellen fremedon. oft scyld scefing sceaþe<add>na</add>
            <lb/>þreatum, moneg<expan>um</expan> mægþum meodo-setl<add>a</add>
            <lb/>of<damage>
              <desc>blot</desc>
            </damage>teah ...</egXML>
          <egXML xmlns="http://www.tei-c.org/ns/Examples">
            <lg>
              <l>Hwæt! we Gar-dena in gear-dagum</l>
              <l>þeod-cyninga þrym gefrunon,</l>
              <l>hu ða æþelingas ellen fremedon,</l>
            </lg>
            <lg>
              <l>Oft <persName>Scyld Scefing</persName> sceaþena þreatum,</l>
              <l>monegum mægþum meodo-setla ofteah;</l>
              <l>egsode <orgName>Eorle</orgName>, syððan ærest wearþ</l>
              <l>feasceaft funden...</l>
            </lg>
          </egXML>
        </p>
      </div>
      <div type="slide">
        <head>... and </head>
        <egXML xmlns="http://www.tei-c.org/ns/Examples">
          <s><w pos="interj" lemma="hwaet">Hwæt</w>
            <w pos="pron" lemma="we">we</w>
            <w pos="npl" lemma="gar-denum">Gar-dena</w>
            <w pos="prep" lemma="in">in</w>
            <w pos="npl" lemma="gear-dagum">gear-dagum</w> ...</s>
        </egXML>
        <p>or even</p>
        <egXML xmlns="http://www.tei-c.org/ns/Examples">
          <w pos="npl" corresp="#w2">Gar-dena</w>
          <w pos="prep" corresp="#w3">in</w>
          <w pos="npl" corresp="#w4">gear-dagum</w>
          <!-- ... -->
          <w xml:id="w2">armed danes</w>
          <w xml:id="w3">in</w>
          <w xml:id="w4">days of yore</w>
        </egXML>
        <p>.. not to mention ... </p>
        <egXML xmlns="http://www.tei-c.org/ns/Examples">
          <!-- ... -->
          <l>Oft <persName ref="https://en.wikipedia.org/wiki/Skj%C3%B6ldr">Scyld Scefing</persName>
            sceaþena þreatum,</l>
        </egXML>
        <p>or even</p>
        <egXML xmlns="http://www.tei-c.org/ns/Examples">
          <l>Oft <persName ref="#skioldus">Scyld Scefing</persName> sceaþena þreatum,</l>
          <!-- ... -->
          <person xml:id="skioldus">
            <persName source="#beowulf">Scyld Scefing</persName>
            <persName xml:lang="lat">Skioldus</persName>
            <persName xml:lang="non">Skjöld</persName>
            <occupation>Legendary Norse King</occupation>
            <ref target="https://en.wikipedia.org/wiki/Skj%C3%B6ldr">Wikipedia entry</ref>
            <!-- ... -->
          </person>
        </egXML>
      </div>

      <div type="slide">
        <head>Choices (2) </head>
        <p>How about this kind of object ... </p>

        <figure>
          <graphic url="media/londonLib.jpg" height="80%"/>
          <head>A random shelf from the London Library</head>
        </figure>
      </div>
      <div type="slide">
        <head>The digital library model</head>
        <p rend="box">What can you can do with a million books?</p>
        <list>
          <item>Text is a bunch of page images backed up with OCR-generated transcription <list>
              <item>analysable only as a bag of words</item>
            </list></item>
          <item>A mass of bibliographical data <list>
              <item>begging the representativeness question</item>
            </list></item>
        </list>
      </div>
      <div type="slide">
        <head>Distant Reading</head>
        <figure>
          <graphic url="media/canonViz.png" height="75%"/>
          <head>(From Ryan Heuser on twitter recently)</head>
        </figure>
        <p rend="box">"Designing a text-analysis program is necessarily an interpretative act, not a
          mechanical one, even if running the program becomes mechanistic." (Joanna Drucker,
            <title>Why Distant Reading
          Isn’t</title><!--,” PMLA 132.3 (May -->(2017)<!--, 631.)--></p>
      </div>

      <!-- 

the double-edged sword of data shows just how important
it is to understand how structures of power and privilege operate in the world.
Catherine D’Ignazio and Lauren Klein, “Introduction,” Data Feminism. Public comment draft (MIT Press, 2018)
n.p. https://bookbook.pubpub.org/pub/dgv16l22.



-->
      <div type="slide">
        <head>Choices (3) </head>
        <p>... or this kind of object </p>
        <figure>
          <graphic url="media/bigData.jpg" height="80%"/>
          <head/>
        </figure>
      </div>
      <div type="slide">
        <head>The linked data model</head>
        <figure>
          <graphic url="media/LODmodel.png" height="70%"/>
          <head>Hype?</head>
        </figure>

        <p rend="box"><q>LOD creates a store of machine-actionable data on which improved services
            can be built... facilitate the breakdown of the tyranny of domain silos ... provide
            direct access to data in ways that are not currently possible ... provide unanticipated
            benefits that will emerge later </q> (Anon, passim) </p>

        <p>LOD is about linking web pages together... </p>
        <list>
          <item>The "meaning" of a set of TEI documents may be inherently complex, nuanced,
            internally contradictory, imprecise.</item>
          <item>The "meaning" of a web page supporting a bit of e-commerce is exhausted by its RDF
            description</item>
        </list>
      </div>



      <div type="slide">
        <head>Wait ... </head>
        <list>
          <item>Just how many markup systems/models does the world need?<list>
              <item>One size fits all?</item>
              <item>Let a thousand flowers bloom?</item>
              <item>Roll your own! </item>
            </list></item>
          <item>We've been here before...<list>
              <item>one construct and many views</item>
              <item>modularity and extensibility</item>
            </list></item>
        </list>
        <p rend="box">... did someone mention the TEI ?</p>
      </div>



      <div type="slide">
        <head>Impact and effects of data-driven research</head>
        <list>
          <item>The trend towards open data motivated (partly) by scientistic replicability</item>
          <item>The digital demotic : opening up of interdisciplinary possibilities -- and the cult
            of the amateur</item>
          <item>Some specific methodological considerations: <list>
              <item>what are the underlying populations being sampled? </item>
              <item>what kind/s of standardisation work best? </item>
              <item>how reliably can disparate sources be analysed together?</item>
            </list></item>
        </list>
      </div>


      <div type="slide">
        <head>Representativeness ... of what?</head>
        <p>Are there more novels published by men than by women in the 19th century? </p>
        <figure>
          <graphic url="media/authorship.png"/>
          <head>Data from http://www.victorianresearch.org/atcl/</head>
        </figure>
        <p>How should we create a representative sample of this population?</p>
        <list>
          <item>by number (1 each from the first decade, 8 each from the last)?</item>
          <item>by variability (1 each from each decade)</item>
        </list>
        <p>But what if we want to consider multiple categories? </p>
      </div>
      <div type="slide">
        <head>Aiming for a balanced corpus </head>
        <figure>
          <head>English</head>
          <graphic url="media/balance-eng.png"/>
          <head>Data from <ref target="http://distantreading.github.io/ELTeC/"
              >http://distantreading.github.io/ELTeC/</ref></head>
        </figure>
        <cb/>
        <p>cultural difference or sampling error? </p>
        <figure>
          <head>Hungarian</head>
          <graphic url="media/balance-hun.jpg"/>
        </figure>
      </div>



      <div type="slide">
        <head>Is the TEI really an ontology?</head>

        <list>
          <item>The TEI was originally conceived as a way of modelling the concepts researchers
            shared about the nature of the texts they wished to process</item>
          <item>Modelling the semantics of that set of concepts formally remains a research project </item>
          <item>Crosswalks or mappings to other "real" ontologies such as OWL are possible: one has
            been implemented for CIDOC-CRM</item>
          <item>(the TEI provides a hook in the shape of the <gi>equiv</gi> element) </item>
        </list>
        <p rend="box">But before we can extract or model their content, documents must be
          interpreted... </p>
      </div>
      <div type="slide">
        <head>Data science and textual data</head>
        <p rend="box">.... and (maybe) the same applies to data</p>

        <quote>"All data is historical data: the product of a time, place, political, economic,
          technical, &amp; social climate. If you are not considering why your data exists, and
          other data sets don’t, you are doing data science wrong” </quote>
        <!--  <quote>Les données ont toujours un aspect historique - elles ressortent d'un temps, d'un
          endroit, ou d'un climat socio-économique spécifique.  Si vous ne mettez pas en question les raisons
          pour lesquelles vos données à vous existent et d'autres sont disparues, alors
          vous pratiquez incorrectement la science de données</quote>
-->
        <p> [Melissa Terras, Opportunities, barriers, and rewards in digitally-led analysis of
          history, culture and society. Turing Lecture 2019-03-03, <ptr
            target="https://youtu.be/4yYytLUViI4"/>] </p>
      </div>
      <div type="slide">
        <head>A recent example: global reach versus situated context</head>
        <list>
          <item>A nice simple well-understood domain : digital collections of 19th century
            newspapers</item>
          <item>all professionally catalogued with extensive metadata</item>
          <item>all (mostly) accessible via standard interfaces</item>
        </list>
        <p rend="box">"attempts to create a single map of all possible elements and attributes, and
          to provide provenance of internal structures while grouping object by type and subtype,
          raised significant ontological issues" (Beals, M. H. et al <title>The Atlas of Digitised
            Newspapers and Metadata</title>, 2020) </p>
        <!-- <p>Beals, M. H. et al The Atlas of Digitised Newspapers and Metadata: Reports from Oceanic
          Exchanges. Loughborough University, 2020. doi: 10.6084/m9.figshare.11560059</p>
        <p>Contrasts with https://github.com/OceanicExchanges/NewspaperMetadata (9 fields, 2 added
          lat and long)</p>-->
        <p>Different institutional catalogues<list>
            <item>describe the same items with differing degrees of completeness</item>
            <item>use similar but not identical terminology</item>
          </list></p>
        <p>All institutional collections<list>
            <item>reflect historically situated selection principles </item>
            <item>in particular, selection for digitization is motivated largely by economic
              considerations</item>
          </list></p>
      </div>
      <div type="slide">
        <head>Conclusion</head>
        <figure>
          <graphic url="media/behindDoorsUmbertoEco.jpg" height="50%"/>
          <head>Still from <title>Behind the Doors</title> (Wehn-Damisch,2012)</head>
        </figure>
        <p>Umberto says:</p>
        <list>
          <item>In spite of the obvious differences in degrees of certainty and uncertainty, every
            picture of the world (be it a scientific law or a novel) is a book in its own right,
            open to further interpretation. </item>
          <item>But certain interpretations can be recognised as unsuccessful because they are like
            a mule, that is, they are unable to produce new interpretations or cannot be confronted
            with the traditions of the previous interpretation. </item>
          <item>The force of the Copernican revolution is not only due to the fact that it explains
            some astronomical phenomena better than the Ptolemaic tradition, but also to the fact
            that – instead of representing Ptolemy as a crazy liar – it explains why and on which
            grounds he was justified in outlining his own interpretation</item>
        </list>
        <p>We conclude...</p>
        <p rend="box"><quote>Text is not a special kind of data: data is a special kind of
            text</quote>
        </p>
      </div>
    </body>
    <!--

      <div type="slide">
        <head>TEI and Linked Open Data</head>
        <p>LOD and TEI share the same goals: data sharing, open resources, etc. Is RDF, the
          currently preferred technology for supporting LOD, usable within TEI resources? </p>
        <list>
          <item>The "meaning" of a set of TEI documents may be inherently complex, nuanced,
            internally contradictory, imprecise.</item>
          <item>The "meaning" of a web page supporting a bit of e-commerce is exhausted by its RDF
            description</item>
          <item>Is the TEI model a formal ontology?</item>
        </list>

        <p>Nevertheless, several ways of integrating RDF descriptions into TEI documents have been
          proposed:</p>
        <list>
          <item>directly, using the RDF namespace (possibly wrapped in <gi>xenodata</gi>) </item>
          <item>indirectly by means of a documented mapping/cross-walk (works for Dublin
            Core)</item>
          <item>using the element <gi>tei:relation</gi>
          </item>
          <item>using RDFa attributes</item>
          <item>(ab)using the TEI elements <gi>graph</gi>, ou <gi>link</gi>, ou <gi>fs</gi></item>
        </list>
        <p rend="box">Under discussion since (at least) 2013... for a recent summary see <ptr
            target="https://github.com/TEIC/TEI/issues/1860"/>
        </p>
      </div>-->
    <!-- scholarship depends on continuity
it is not enough to preserve the bytes of an encoding
 there must also be a continuity of comprehension: the encoding must be self-descriptive-->

    <!--  <div type="slide">
        <head>The Text Encoding Initiative</head>
        <list>
          <item>Spring 1987: European workshops on standardisation of historical data (J.P. Genet,
            M. Thaller )</item>
          <item>Autumn 1987: In the US, the NEH funds an exploratory international workshop on the
            feasibility of defining "text encoding guidelines"</item>
        </list>
        <figure>
          <graphic height="70%" url="../Graphics/poughkeepsie.png"/>
          <head>Vassar College, Poughkeepsie</head>
        </figure>
      </div>
-->

    <!--
      <div type="slide">
        <head>The obvious question</head>
        <list>
          <item>The TEI is <emph>very old</emph>! <list>
              <item>Not much in computing survives 5 years, never mind 20</item>
              <item>Why is it still here, and how has it survived?</item>
              <item>What relevance can it possibly have today?</item>
            </list></item>
          <item>And with XML everyone can create their own markup system and still share
            data!</item>
          <item>And in the Semantic Web, XML systems will all understand each other's data!</item>
          <item>RDF can describe every kind of markup; SPARQL can search it! </item>
        </list>
        <p rend="box">Well .... maybe .... </p>

      </div>



      <div type="slide">
        <head>Why the TEI?</head>
        <p>The TEI provides <list rend="pause">
            <item>a language-independent framework for defining markup languages</item>
            <item>a very simple consensus-based way of organizing and structuring textual (and
              other) resources...</item>
            <item>... which can be enriched and personalized in highly idiosyncratic or specialised
              ways</item>
            <item>a very rich library of existing specialised components</item>
            <item>an integrated suite of standard stylesheets for delivering schemas and
              documentation in various languages and formats</item>
            <item>a large and active open source style user community</item>
          </list>
        </p>
      </div>

      <div type="slide">
        <head>Relevance</head>
        <p>Why would you want those things? <list rend="pause">
            <item>because we need to interchange resources <list>
                <item>between people</item>
                <item>(increasingly) between machines</item>
              </list>
            </item>
            <item>because we need to integrate resources <list>
                <item>of different media types</item>
                <item>from different technical contexts</item>
              </list></item>
            <item>because we need to preserve resources <list>
                <item>cryogenics is not the answer!</item>
                <item>we need to preserve metadata as well as data</item>
              </list></item>
          </list></p>
      </div>

      <div type="slide">
        <head>The virtuous circle of encoding</head>
        <p>
          <graphic url="media/model.png"/>
        </p>
      </div>


      <div type="slide">
        <head>The scope of intelligent markup</head>
        <p>The TEI provides -\- amongst others -\- recommended markup for <list>
            <item>basic structural and functional components of text </item>
            <item>diplomatic transcription, images, annotation</item>
            <item>links, correspondence, alignment</item>
            <item>data-like objects such as dates, times, places, persons, events (<term>named
                entity recognition</term>)</item>
            <item>meta-textual annotations (correction, deletion, etc)</item>
            <item>linguistic analysis at all levels</item>
            <item>contextual metadata of all kinds</item>
            <item>... and so on and so on and so forth</item>
          </list></p>
        <p rend="box">Is it possible to delimit encyclopaedically all possible kinds of markup? </p>
      </div>

      <div type="slide">
        <head>Why use a common framework ?</head>
        <list>
          <item>re-usability and repurposing of resources</item>
          <item>modular software development </item>
          <item>lower training costs</item>
          <item><q>frequently answered questions</q> &#x2014; common technical solutions for
            different application areas </item>
        </list>
        <p rend="box">The TEI was <emph>designed</emph> to support multiple views of the same
          resource</p>
      </div>


      <div type="slide">
        <head>Conformance issues</head>

        <p>A document is <term>TEI Conformant</term> if and only if it: <list type="simple">
            <item>is a well-formed XML document</item>
            <item>can be validated against a <term>TEI Schema</term>, that is, a schema derived from
              the TEI Guidelines</item>
            <item> conforms to the TEI Abstract Model </item>
            <item> uses the <term>TEI Namespace</term> (and other namespaces where relevant)
              correctly </item>
            <item> is documented by means of a TEI Conformant <term>ODD file</term> which refers to
              the TEI Guidelines </item>
          </list><!-\- or if it can be transformed automatically using some
          TEI-defined procedures into such a document (it is then considered
          <term>TEI-conformable</term>).-\-></p>

        <p rend="box">TEI conformance does not mean <q>Do what I do</q>, but rather <q>Explain what
            you do in terms I can understand</q></p>
      </div>
      <div type="slide">
        <head>Why is the TEI still here?</head>

        <p rend="box">Because it is a model of textual data which is ... <list>
            <item>customisable, </item>
            <item>self-descriptive, </item>
            <item>and user-driven</item>
          </list></p>
      </div>

-->


    <!-- <div type="slide">
        <head>Where is the textual data and where is the markup?</head>
        <figure>
          <graphic url="media/beowulf-ms.png" height="80%"/>
          <head>BL Ms Cotton Vitelius A xv, fol. 129r</head>
        </figure>
      </div>
      <div type="slide">
        <head>Where is the textual data and where is the markup?</head>
        <figure>
          <graphic url="media/beowulf-wrenn.png" height="70%"/>
          <head>Beowulf, ed. C L Wrenn (with student annotations)</head>
        </figure>
      </div>-->
  </text>
</TEI>
