<?xml version="1.0" encoding="UTF-8"?><TEI xmlns="http://www.tei-c.org/ns/1.0"><teiHeader><fileDesc><titleStmt><title>ISO-TEI Transcription of multimodal resources.docx</title><author/></titleStmt><editionStmt><edition><date/></edition></editionStmt><publicationStmt><p>unknown</p></publicationStmt><sourceDesc><p>Converted from a Word document </p></sourceDesc></fileDesc><encodingDesc><appInfo><application ident="TEI_fromDOCX" version="2.15.0"><label>DOCX to TEI</label></application></appInfo></encodingDesc><revisionDesc><change><date>$LastChangedDate: 2013-06-16T16:57:03Z$</date><name/></change></revisionDesc></teiHeader><text><body><div><head/><p rend="Subtitle"><anchor xml:id="h.mn2di3p4xxmx"/>New work item proposal</p><p rend="Title"><anchor xml:id="h.7tmlldashgp1"/>Transcription of Spoken Language</p><p>Coordinating author: Thomas Schmidt (<ref target="mailto:thomas.schmidt@ids-mannheim.de"><hi rend="underline color(1155cc)">thomas.schmidt@ids-mannheim.de</hi></ref>)</p><p>Document history: </p><p>22-October-2012: 	1st draft for ISO/DIN Meeting in Berlin, based on jTEI Paper</p><p>18-March-2013:		2nd draft for ISO/DIN Meeting in Potsdam, modifications and additions </p><p>according to discussion in Berlin and comments from the French group</p></div><div xml:id="h.hagawfj907ym"><head>Table of Contents</head><p><ref target="#h.hagawfj907ym"><hi rend="underline color(1155cc)">Table of Contents</hi></ref></p><p><ref target="#h.7degqrm52wou"><hi rend="underline color(1155cc)">Experts</hi></ref></p><p><ref target="#h.yysh5i7gq8pk"><hi rend="underline color(1155cc)">Introduction</hi></ref></p><p><ref target="#h.qyn0r4puh88a"><hi rend="underline color(1155cc)">1 Scope</hi></ref></p><p><ref target="#h.yfg9qbw4veac"><hi rend="underline color(1155cc)">2 Normative references</hi></ref></p><p><ref target="#h.k8lhntxmgnlv"><hi rend="underline color(1155cc)">3 Terms and definitions</hi></ref></p><p><ref target="#h.mxd7fdb9hf8f"><hi rend="underline color(1155cc)">4 General document structure</hi></ref></p><p><ref target="#h.tp11p16vrk1o"><hi rend="underline color(1155cc)">5 Metadata for the transcription of multimodal corpora</hi></ref></p><p><ref target="#h.g4ls38l1aug"><hi rend="underline color(1155cc)">5.1. fileDesc</hi></ref></p><p><ref target="#h.k3tmktpev7ms"><hi rend="underline color(1155cc)">5.1.1. publicationStmt</hi></ref></p><p><ref target="#h.u8r4mr3tw2on"><hi rend="underline color(1155cc)">5.1.2 recordingStmt</hi></ref></p><p><ref target="#h.7e144u94vjs1"><hi rend="underline color(1155cc)">5.2. profileDesc</hi></ref></p><p><ref target="#h.m6wjj4g2ebgj"><hi rend="underline color(1155cc)">5.2.1. particDesc</hi></ref></p><p><ref target="#h.jkdr948sajnl"><hi rend="underline color(1155cc)">5.2.2. settingDesc</hi></ref></p><p><ref target="#h.g58o0g7l3can"><hi rend="underline color(1155cc)">5.3. encodingDesc</hi></ref></p><p><ref target="#h.jokwrjg8j1vs"><hi rend="underline color(1155cc)">6 Macrostructure</hi></ref></p><p><ref target="#h.dfbr5b22grbz"><hi rend="underline color(1155cc)">6.1 Characterisation in terms of annotation graphs</hi></ref></p><p><ref target="#h.dfbr5b22grbz"><hi rend="underline color(1155cc)">6.2 Timeline (&lt;timeline&gt;)</hi></ref></p><p><ref target="#h.31tvgqh77j5"><hi rend="underline color(1155cc)">6.3 Utterances (&lt;u&gt;)</hi></ref></p><p><ref target="#h.4v12ridd6jc5"><hi rend="underline color(1155cc)">6.4 Dependent annotations (&lt;spanGrp&gt;)</hi></ref></p><p><ref target="#h.80clzsvnajnb"><hi rend="underline color(1155cc)">6.5 Grouping of utterances and dependent annotations (&lt;div&gt;)</hi></ref></p><p><ref target="#h.dfbr5b22grbz"><hi rend="underline color(1155cc)">6.6 Independent elements outside utterances</hi></ref></p><p><ref target="#h.cch0wsj32244"><hi rend="underline color(1155cc)">7 Microstructure</hi></ref></p><p><ref target="#h.lxgbc8fwubin"><hi rend="underline color(1155cc)">7.1 Words</hi></ref></p><p><ref target="#h.aqf1hmp37zy"><hi rend="underline color(1155cc)">7.1.1 Characterisation</hi></ref></p><p><ref target="#h.1eypzalj545n"><hi rend="underline color(1155cc)">7.1.2 Representation as &lt;w&gt;</hi></ref></p><p><ref target="#h.7kfy6iwvrix9"><hi rend="underline color(1155cc)">7.1.3 Further constraints</hi></ref></p><p><ref target="#h.sdziubp2fac9"><hi rend="underline color(1155cc)">7.1.4 Examples</hi></ref></p><p><ref target="#h.z3o3yr42f496"><hi rend="underline color(1155cc)">7.2 Pauses</hi></ref></p><p><ref target="#h.x2cshg87ru4t"><hi rend="underline color(1155cc)">7.2.1 Characterisation</hi></ref></p><p><ref target="#h.33gkhoe4mvlj"><hi rend="underline color(1155cc)">7.2.2 Representation as &lt;pause&gt;</hi></ref></p><p><ref target="#h.b8a9llpufup4"><hi rend="underline color(1155cc)">7.2.3 Further constraints</hi></ref></p><p><ref target="#h.1yclhttdt8hs"><hi rend="underline color(1155cc)">7.2.4 Examples</hi></ref></p><p><ref target="#h.bbkr4lki56qj"><hi rend="underline color(1155cc)">7.3 Audible non-speech events</hi></ref></p><p><ref target="#h.bplr5t6cwkf"><hi rend="underline color(1155cc)">7.3.1 Characterisation</hi></ref></p><p><ref target="#h.jk6li6g4qn6v"><hi rend="underline color(1155cc)">7.3.2 Representation as &lt;vocal&gt;, &lt;kinesic&gt; or &lt;incident&gt;,</hi></ref></p><p><ref target="#h.9lqsmgrbsjcc"><hi rend="underline color(1155cc)">7.3.3 Further constraints</hi></ref></p><p><ref target="#h.o0t7l0xrs9ed"><hi rend="underline color(1155cc)">7.3.4 Examples</hi></ref></p><p><ref target="#h.vdzrkhs7l0s2"><hi rend="underline color(1155cc)">7.4 Punctuation</hi></ref></p><p><ref target="#h.2xy9getlagg5"><hi rend="underline color(1155cc)">7.4.1 Characterisation</hi></ref></p><p><ref target="#h.wbf984n48hgc"><hi rend="underline color(1155cc)">7.4.2 Representation as &lt;c&gt; (or &lt;pc&gt;?)</hi></ref></p><p><ref target="#h.h0bz0y1zdq7z"><hi rend="underline color(1155cc)">7.4.3 Further constraints</hi></ref></p><p><ref target="#h.bm4hbmm13f0v"><hi rend="underline color(1155cc)">7.4.4 Examples</hi></ref></p><p><ref target="#h.19xwnk4qpcrs"><hi rend="underline color(1155cc)">7.5 Uncertainty and incomprehensible passages</hi></ref></p><p><ref target="#h.zigkq2ao9brr"><hi rend="underline color(1155cc)">7.5.1 Characterisation</hi></ref></p><p><ref target="#h.9qjwz3kvltva"><hi rend="underline color(1155cc)">7.5.2 Representation as &lt;unclear&gt;</hi></ref></p><p><ref target="#h.3qi69r7pwi08"><hi rend="underline color(1155cc)">7.5.3 Further constraints</hi></ref></p><p><ref target="#h.8jyhcow71dsl"><hi rend="underline color(1155cc)">7.5.4 Examples</hi></ref></p><p><ref target="#h.gvsavu68whzo"><hi rend="underline color(1155cc)">7.6 Units above the word and below the &lt;u&gt; level</hi></ref></p><p><ref target="#h.sv2zrql6ctw5"><hi rend="underline color(1155cc)">7.6.1 Characterisation</hi></ref></p><p><ref target="#h.87orfhd8c8ry"><hi rend="underline color(1155cc)">7.6.2 Representation as &lt;seg&gt;</hi></ref></p><p><ref target="#h.vyllcplp9hbj"><hi rend="underline color(1155cc)">7.6.3 Further constraints</hi></ref></p><p><ref target="#h.1u333zjazr4s"><hi rend="underline color(1155cc)">7.6.4 Examples</hi></ref></p><p><ref target="#h.mr3zkmijs0p1"><hi rend="underline color(1155cc)">8 Bibliographical reference</hi></ref></p><p><ref target="#h.wn7z4lufnxr7"><hi rend="underline color(1155cc)">9 Annex</hi></ref></p><p><ref target="#h.yaezeltxcvgx"><hi rend="underline color(1155cc)">10 Annex - fully encoded example</hi></ref></p><p><ref target="#h.y08vdpyda3nq"><hi rend="underline color(1155cc)">11 Annex(es)</hi></ref></p><p><ref target="#h.t304ji4idlqo"><hi rend="underline color(1155cc)">12 Annex(es)</hi></ref></p><p><ref target="#h.lw3w0tgd5m6q"><hi rend="underline color(1155cc)">Minutes ISO/DIN Meeting Berlin, 22-October-2012</hi></ref></p><p><ref target="#h.lidhcauvwusg"><hi rend="underline color(1155cc)">Comments Carol Etienne (by e-mail 11-February-2013)</hi></ref></p></div><div xml:id="h.cqkqs7z9o906"><head/><p><pb/></p></div><div xml:id="h.7degqrm52wou"><head>Experts</head><p>Core Group:</p><list type="unordered"><item>Lou Burnard</item><item>Bertrand Gaiffe</item><item>Laurent Romary</item><item>Thomas Schmidt</item><item>Andreas Witt</item></list><p>EIT / MMI Group:</p><list type="unordered"><item>Nadia Mana from FBK (Trento, Italy)</item><item>Tatjana Scheffler (DFKI, Germany)</item><item>Khiet Truong (Univ of Twente)</item><item>Benjamin Weiss (TU Berlin)</item><item>Mathias Wilhelm (DAI Labor)</item><item>Felix Burkhardt (Deutsche Telekom's Innovation Lab)</item></list><p>Working group 'Interopérabilité' of the Consortium ‘Corpus Oraux et multimodaux (IRCOM)’</p><list type="unordered"><item>Coordinators: Christophe Parisse, Carole Etienne, Bernard Bel.</item><item>Participants : Sarra El Ayari, Nicolas Ballier, Christophe Benzitoun, Philippe Blache, Annelies Braffort, Christian Chanard, Dominique Fohr, Séverine Guillaume, Clément Planck.</item></list><p>Tool developers (not yet contacted):</p><list type="unordered"><item>Michael Kipp (FH Augsburg, developer of ANVIL)</item><item>Han Sloetjes (MPI Nijmegen, developer of ELAN)</item><item>Kai Wörner (Hamburg, co-developer of EXMARaLDA)</item><item>(To be contacted: Praat, Paul Boersms, Transcriber: Edouard Geoffrois </item></list><p>Other experts (not yet contacted):</p><list type="unordered"><item>Ulrike Gut (Münster, Corpus Phonology Network)</item><item>Florian Schiel, Christoph Draxler (München, Bavarian Archive for Sound Signals)</item></list></div><div xml:id="h.q78aws7v2oa7"><head/><p><pb/></p></div><div xml:id="h.yysh5i7gq8pk"><head>Introduction</head><p>Context, multiple tools, multiple formats, multiple transcription conventions cf. Intro TS-jTEI</p><p>Complementarity with specific annotation levels MAF, SynAF, SemAF</p><p>Joint ISO-TEI initiative (cf. MoU)</p><p>Legacy data vs. new data/tools</p></div><div xml:id="h.qyn0r4puh88a"><head>1 Scope</head><p>This standard aims at enabling and facilitating the interchange of transcriptions of spoken language between different computational tools and environments for creating, editing, publishing and exploiting such data. Typically, transcription of spoken language here means an orthography-based transcription of verbal activity as recorded in an audio or video recording of a natural interaction. The description of activity in other modalities (e.g. gestures, mimics) may be part of a spoken language transcription, but the standard departs from the assumption that the verbal dimension is the primary focus of a spoken language transcription. </p><p>The standard takes into account data models and encoding practices supported by widely used transcription editors (such as Transcriber, ELAN, EXMARaLDA etc.) and aims at being compatible with the formats produced by these tools. </p></div><div xml:id="h.yfg9qbw4veac"><head>2 Normative references</head><p>XML</p><p>Maybe only ISO documents to be cited - TEI P5 (version?)</p></div><div xml:id="h.k8lhntxmgnlv"><head>3 Terms and definitions</head><p>transcription, annotation, transcription convention, transcription tool, tier, …</p></div><div xml:id="h.mxd7fdb9hf8f"><head>4 General document structure</head><p>One, multiple documents? – TEICorpus?</p><p>primary - subordinate</p><p>tier</p><p>metadata</p></div><div xml:id="h.tp11p16vrk1o"><head>5 Metadata </head><p>The TEI Guidelines formulates extensive suggestions for encoding metadata inside different subsections of the &lt;teiHeader&gt; element. In the following, only those pieces of metadata are addressed which are crucial for ensuring the interpretability and exchangeability of spoken language transcriptions in general. This does not preclude the possibility of or necessity for encoding further metadata inside the &lt;teiHeader&gt; element.</p><div xml:id="h.c4bc0jecfh5n"><head>5.1. fileDesc</head><p>Text </p><div xml:id="h.k3tmktpev7ms"><head>5.1.1. publicationStmt</head><p>The &lt;publicationStmt&gt; element inside the &lt;fileDesc&gt; section of the &lt;teiHeader&gt; should be used to record information about access rights and contact information for the transcription in question.</p><table rend="rules"><row><cell><p><hi rend="color(333333)">&lt;publicationStmt&gt;</hi></p><p><hi rend="color(333333)">   &lt;authority&gt;Hamburger Zentrum für Sprachkorpora&lt;/authority&gt;</hi></p><p><hi rend="color(333333)">   &lt;availability&gt;Available free for research and teaching purposes&lt;/availability&gt;</hi></p><p><hi rend="color(333333)">   &lt;distributor&gt;Hamburger Zentrum für Sprachkorpora&lt;/distributor&gt;</hi></p><p><hi rend="color(333333)">   &lt;address&gt;</hi></p><p><hi rend="color(333333)">      &lt;street&gt;Max Brauer-Allee 60&lt;/street&gt;</hi></p><p><hi rend="color(333333)">      &lt;postCode&gt;22765&lt;/postCode&gt;</hi></p><p><hi rend="color(333333)">      &lt;placeName&gt;Hamburg&lt;/placeName&gt;</hi></p><p><hi rend="color(333333)">      &lt;country&gt;Germany&lt;/country&gt;</hi></p><p><hi rend="color(333333)">   &lt;/address&gt;</hi></p><p><hi rend="color(333333)">&lt;/publicationStmt&gt;</hi></p></cell></row></table></div><div xml:id="h.u8r4mr3tw2on"><head>5.1.2 recordingStmt</head><p>The &lt;publicationStmt&gt; element inside the &lt;fileDesc&gt; section of the &lt;teiHeader&gt; should be used to record information about the transcribed recording(s). A &lt;media&gt; element should be used to refer to the corresponding digital file. Where two or more files are derived from the same master recording, these should be represented as different &lt;media&gt; elements inside the same &lt;recording&gt; element, rather than as different &lt;recording&gt; elements. </p><p><hi rend="color(0000ff)">Comment CE: &lt;recording&gt; and &lt;equipment&gt;</hi><note place="comment" resp=""><date when=""/><hi rend="italic normalweight baseline strikethrough smallcaps">carole.etienne:may be to extend because we lack main informations interesting to identify  like quality, anonymization or streaming</hi></note><hi rend="color(0000ff)"> to define media files quality, a set of extracts of the same record in different format or quality, available streaming </hi></p><table rend="rules"><row><cell><p><hi rend="color(333333)">&lt;recordingStmt&gt;</hi></p><p><hi rend="color(333333)">     &lt;recording type="video"&gt;</hi></p><p><hi rend="color(333333)">        &lt;!-- element from TEI P5, but not allowed there as a child of recording --&gt;</hi></p><p><hi rend="color(333333)">        &lt;media mimeType="video/mpeg" url="Beckhams.mpg"/&gt;</hi></p><p><hi rend="color(333333)">        &lt;media mimeType="audio/wav" url="Beckhams.wav"/&gt;</hi></p><p><hi rend="color(333333)">        &lt;broadcast&gt;</hi></p><p><hi rend="color(333333)">           &lt;ab&gt;Parkinson Talkshow on BBC, broadcast on 02 November 2007&lt;/ab&gt;</hi></p><p><hi rend="color(333333)">        &lt;/broadcast&gt;</hi></p><p><hi rend="color(333333)">        &lt;!-- information about the equipment used for creating the recording --&gt;</hi></p><p><hi rend="color(333333)">        &lt;!-- where recordings are made by the researcher, this would be --&gt;</hi></p><p><hi rend="color(333333)">        &lt;!-- place to specify the recording equipment (e.g. Camcorder) --&gt;</hi></p><p><hi rend="color(333333)">        &lt;equipment&gt;</hi></p><p><hi rend="color(333333)">           &lt;ab&gt;Video excerpt downloaded from YouTube with aTube-Catcher, converted </hi></p><p><hi rend="color(333333)">               into MPG format with Adobe Premiere&lt;/ab&gt;</hi></p><p><hi rend="color(333333)">           &lt;ab&gt;Audio extracted from video with Audacity 1.3 beta&lt;/ab&gt;</hi></p><p><hi rend="color(333333)">        &lt;/equipment&gt;                  </hi></p><p><hi rend="color(333333)">     &lt;/recording&gt;</hi></p><p><hi rend="color(333333)">&lt;/recordingStmt&gt;</hi></p></cell></row></table></div></div><div xml:id="h.7e144u94vjs1"><head>5.2. profileDesc</head><div xml:id="h.m6wjj4g2ebgj"><head>5.2.1. particDesc</head><p>The participants of the transcribed interaction should be described in &lt;person&gt; elements inside the &lt;particDesc&gt; section of a &lt;profilDesc&gt; element. Using the &lt;abbr&gt; element to define an abbreviated code for the respective participant can be crucial for many processing purposes. &lt;u&gt; elements inside the body of the transcription refer to the @id attribute of a &lt;person&gt; element which should therefore always be provided.</p><p><hi rend="color(0000ff)">Comment TS: The question came up what to do when it is necessary to identify speakers across individual documents (i.e. where one and the same speaker figures in different transcriptions of the same corpus). Is it a reasonable suggestions to ensure that, in these cases, ids of a given speaker should be identical across documents?</hi></p><table rend="rules"><row><cell><p><hi rend="color(333333)">&lt;particDesc&gt;</hi></p><p><hi rend="color(333333)">   &lt;person xml:id="SPK0" sex="1"&gt;</hi></p><p><hi rend="color(333333)">     &lt;persName&gt;</hi></p><p><hi rend="color(333333)">       &lt;abbr&gt;DS&lt;/abbr&gt;</hi></p><p><hi rend="color(333333)">     &lt;/persName&gt;</hi></p><p><hi rend="color(333333)">     &lt;!-- possibly further descriptive elements --&gt;</hi></p><p><hi rend="color(333333)">   &lt;/person&gt;</hi></p><p><hi rend="color(333333)">   &lt;person xml:id="SPK1" sex="0"&gt;</hi></p><p><hi rend="color(333333)">     &lt;persName&gt;</hi></p><p><hi rend="color(333333)">       &lt;abbr&gt;FB&lt;/abbr&gt;</hi></p><p><hi rend="color(333333)">     &lt;/persName&gt;</hi></p><p><hi rend="color(333333)">   &lt;/person&gt;</hi></p><p><hi rend="color(333333)">&lt;/particDesc&gt;</hi></p></cell></row></table></div><div xml:id="h.jkdr948sajnl"><head>5.2.2. settingDesc</head><p>The &lt;settingDesc&gt; elements should be used to provide information about the spatial organization, artifacts etc. of the transcribed interaction. </p><p><hi rend="color(0000ff)">Comment CE: &lt;setting&gt; and &lt;activity&gt;</hi><note place="comment" resp=""><date when=""/><hi rend="italic normalweight baseline strikethrough smallcaps">carole.etienne:the number of participants could be an attribute</hi></note><hi rend="color(0000ff)"> to describe spatial organization, artefacts, participants tasks (no link with their usual occupation defined in &lt;person&gt;) </hi></p><table rend="rules"><row><cell><p><hi rend="color(333333)">&lt;settingDesc&gt;</hi></p><p><hi rend="color(333333)">   &lt;place&gt;BBC studio London&lt;/place&gt;</hi></p><p><hi rend="color(333333)">   &lt;setting&gt;</hi></p><p><hi rend="color(333333)">      &lt;activity&gt;Talkshow host Michael Parkinson interviewing David and Victoria  </hi></p><p><hi rend="color(333333)">                Beckham about their relationship&lt;/activity&gt;</hi></p><p><hi rend="color(333333)">   &lt;/setting&gt;</hi></p><p><hi rend="color(333333)">&lt;/settingDesc&gt;</hi></p></cell></row></table></div></div><div xml:id="h.g58o0g7l3can"><head>5.3. encodingDesc</head><p>The &lt;encodingDesc&gt; element should be used to record information about the tool which created the transcription and about the convention which was used in transcribing the data.</p><p><hi rend="color(0000ff)">Comment TS: Does TEI provide a more formalized way of referring to transcription conventions? I think it would be better to use something more structured and machine-readable for this purpose.</hi></p><table rend="rules"><row><cell><p><hi rend="color(333333)">&lt;encodingDesc&gt;</hi></p><p><hi rend="color(333333)">   &lt;appInfo&gt;</hi></p><p><hi rend="color(333333)">      &lt;!-- information about the application with which --&gt;</hi></p><p><hi rend="color(333333)">      &lt;!-- the transcription was created --&gt;</hi></p><p><hi rend="color(333333)">      &lt;application ident="EXMARaLDA" version="1.5.1"&gt;</hi></p><p><hi rend="color(333333)">         &lt;label&gt;EXMARaLDA Partitur-Editor&lt;/label&gt;</hi></p><p><hi rend="color(333333)">         &lt;desc&gt;Transcription Tool providing a TEI Export&lt;/desc&gt;</hi></p><p><hi rend="color(333333)">      &lt;/application&gt;</hi></p><p><hi rend="color(333333)">   &lt;/appInfo&gt;       </hi></p><p><hi rend="color(333333)">   &lt;!-- information about the transcription convention used --&gt;</hi></p><p><hi rend="color(333333)">   &lt;!-- might be better to define a more machine-friendly tag --&gt;</hi></p><p><hi rend="color(333333)">   &lt;!-- for this, e.g. in analogy to appInfo --&gt;</hi></p><p><hi rend="color(333333)">   &lt;!-- &lt;transcriptionConvention ident="HIAT" version="2004" --&gt;</hi></p><p><hi rend="color(333333)">   &lt;p&gt;Orthographic transcription according to HIAT&lt;/p&gt;</hi></p><p><hi rend="color(333333)">&lt;/encodingDesc&gt;</hi></p></cell></row></table></div></div><div xml:id="h.jokwrjg8j1vs"><head>6 Macrostructure</head><div xml:id="h.dfbr5b22grbz"><head>6.1 Characterisation in terms of annotation graphs</head><p>(see also transcription graphs in Schmidt 2005 and the discussion in Schmidt et al. 2009)</p><p><hi rend="color(0000ff)">Question TS: Is this section useful? Does it make sense in a standardisation document  to relate the markup standardisation to a more abstract algebraic framework? If yes, I will elaborate this section. If no, I will remove it.</hi><note place="comment" resp=""><date when=""/><hi rend="italic normalweight baseline strikethrough smallcaps">Bernd Moos:Pretoria meeting: this is background information, should go somewhere else - into the introduction</hi></note></p><p><hi rend="color(434343)">Annotation graphs as in Bird/Liberman (2001) plus:</hi></p><list type="unordered"><item><hi rend="color(434343)">graph must be </hi><hi rend="italic color(434343)">fully anchored</hi></item><item><hi rend="color(434343)">nodes must be </hi><hi rend="italic color(434343)">fully ordered</hi></item><item><hi rend="color(434343)">arcs can (but need not) be assigned to one member of a set of </hi><hi rend="italic color(434343)">speakers</hi></item><item><hi rend="color(434343)">each arc must be assigned to exactly one member of a set of </hi><hi rend="italic color(434343)">categories</hi></item><item><hi rend="color(434343)">exactly one category is assigned to </hi><hi rend="italic color(434343)">type </hi><hi rend="color(434343)">T(ranscription), the remaining categories are assigned to either type A(nnotation) or D(escription)</hi></item><item><hi rend="color(434343)">arcs assigned to categories of types T or A must also be assigned to a speaker</hi></item><item><hi rend="color(434343)">partition of arcs:</hi><list type="unordered"><item><hi rend="color(434343)">all arcs assigned to the same speaker and the category of type ‘T’ → main tier for that speaker, constraint: no overlapping arcs (typically: orthographic transcription)</hi></item><item><hi rend="color(434343)">all arcs assigned to the same speaker and one of the categories of type ‘A’ → dependent tier(s) for that speaker, constraint: corresponding arcs in main tier of the same speaker (typically: linguistic annotation)</hi></item><item><hi rend="color(434343)">all arcs assigned to the same speaker and one of the categories of type ‘D’ → independent secondary tier(s) for that speaker (typically: descriptions of non-verbal behaviour)</hi></item><item><hi rend="color(434343)">a (maximally?) contiguous set of arcs in a main tier → segment chain → &lt;u&gt; element</hi></item></list></item></list></div><div xml:id="h.dfbr5b22grbz"><head>6.2 Timeline (&lt;timeline&gt;)</head><p><hi rend="color(0000ff)">Comment/Question TS: change in revision #2: use relative offsets (interval) instead of absolute. I am still not sure how to define the first &lt;when&gt; element which represents the start of the recording.</hi></p><p>&lt;when&gt; elements inside a &lt;timeline&gt; element should be used to define points in the recording which are then referred to by @start, @end and @synch attributes of other elements of the trancription to represent its temporal strcuture. It is therefore obligatory to provide an @id attribute for each &lt;when&gt; element. &lt;when&gt; elements must be in the same order as the timepoints they refer to. Specifying an absolute offset into the recording via an @interval attribute is optional, but very useful for many processing purposes.</p><table rend="rules"><row><cell><p>&lt;timeline unit="s" origin="#T0"&gt;</p><p>&lt;when xml:id="T0" absolute="00:00:00"/&gt;</p><p>&lt;when xml:id="T1" interval="02.13" since="#T0"/&gt;</p><p>&lt;when xml:id="T2" interval="03.74" since="#T0"/&gt;</p><p>&lt;when xml:id="T3" interval="04.71" since="#T0"/&gt;</p><p>&lt;when xml:id="T4" interval="unknown" since="#T0"/&gt;</p><p>&lt;when xml:id="T5" interval="08.53" since="#T0"/&gt;</p><p>&lt;when xml:id="T6" interval="11.36" since="#T0"/&gt;</p><p>&lt;when xml:id="T7" interval="13.91" since="#T0"/&gt;</p><p>&lt;when xml:id="T8" interval="15.47" since="#T0"/&gt;</p><p>&lt;!-- [...] more when elements --&gt;</p><p>&lt;/timeline&gt;</p></cell></row></table></div><div xml:id="h.31tvgqh77j5"><head>6.3 Utterances (&lt;u&gt;)</head><p>The &lt;u&gt; element is the fundamental unit of organization for a transcription, roughly comparable to a paragraph (&lt;p&gt; element) in a written document. It corresponds to a contiguous stretch of speech of a single speaker. A more exact definition and delimitation of a &lt;u&gt; is not in the scope of this document; the TEI definition characterising a &lt;u&gt; as “often preceded by a silence or a change of speaker” should be viewed as a suggestion only, i.e. it is permissible to use a more refined definition for a &lt;u&gt;.</p><p>&lt;u&gt; elements must be assigned to a single speaker by providing a value for the @who attribute which points to the @id of a &lt;person&gt; element defined in the header.</p><p>&lt;u&gt; elements must be assigned to the timeline by providing values for the @start and @end attributes pointing to the the @id of a &lt;when&gt; element defined in the timeline.</p><p>Further temporal structure can be recorded by inserting &lt;anchor&gt; elements at appropriate places inside the content of a &lt;u&gt; element. </p><p>The preferred mechanism for representing overlap is to encode it implicitly through the appropriate use of @start and @end attributes and &lt;anchor&gt; elements. Other mechanisms, such as a @trans=’overlap’ attribute for the &lt;u&gt; element can be used instead or in addition, but can not be processed in an appropriate manner by many of the widely used annotation tools.</p><p><hi rend="color(0000ff)">Comment CE: In a record including a lot of overlaps (800 overlaps in an hour in some settings), is it possible to define two scales to avoid splitting timeline :</hi></p><p><hi rend="color(0000ff)">. a timeline for timecode with @absolute as 6.2 example and in utterances an &lt;anchor synch="#Tnn"&gt; </hi></p><p><hi rend="color(0000ff)">- an anchor identified with &lt;anchor xml:id="XX"&gt; at the starting point of an overlapped segment assigned to a speaker A and a corresponding anchor tag &lt;anchor synch="#XX"&gt; at the starting point of the overlapping segment assigned to a speaker B to synchronize or link the two segments without time identification if we haven't time-aligned each overlap</hi></p><table rend="rules"><row><cell><p>&lt;!-- u with start and end attributes only (minimal temporal strcuture) --&gt;</p><p>&lt;u who="#SPK1" start="#T0" end="#T1"&gt;Good morning! &lt;/u&gt;</p><p>&lt;!-- u with embedded anchor elements (additional temporal structure) --&gt;</p><p>&lt;u who="#SPK0" start="#T1" end="#T4"&gt;</p><p>  Okay. &lt;anchor synch="#T2"/&gt;Très bien, &lt;anchor synch="#T3"/&gt;très bien. </p><p>&lt;/u&gt;</p><p>&lt;!-- two u’s with partial overlap →</p><p>&lt;u who="#SPK0" start="#T0" end="#T2"&gt;Do not &lt;anchor synch="#T1"/&gt; interrupt me!&lt;/u&gt;</p><p>&lt;u who="#SPK1" start="#T1" end="#T3"&gt;Sorry, &lt;anchor synch="#T2"/&gt; mate!&lt;/u&gt;</p></cell></row></table></div><div xml:id="h.4v12ridd6jc5"><head>6.4 Dependent annotations (&lt;spanGrp&gt;)</head><p>Whereas &lt;u&gt; contains the basic orthographic transcription, &lt;span&gt; elements should be used to represent additional annotations (e.g. POS tagging, prosodic annotation, translation) on that basic transcription. Annotations of the same type should be grouped in a &lt;spanGrp&gt; element with a @type attribute specifying the annotation level.</p><p>The reference of the annotation in question can be specified using @to and @from attributes in two ways: </p><list type="unordered"><item>the values of @to and @from can point to the @id attributes of other elements (e.g. a &lt;u&gt; or a &lt;w&gt;) of the transcription</item><item>the values of @to and @from can point to the @id attributes of &lt;when&gt; elements from the timeline</item></list><p>If the latter mechanism is used, &lt;spanGrp&gt; elements must be grouped with the &lt;u&gt; element they refer to by using an &lt;annotatedU&gt; element (see below).</p><p><hi rend="color(0000ff)">Comment TS: My original proposal was to always group spanGrp with the &lt;u&gt; element they belong to. In the Berlin meeting, it was felt that this should be made optional. However, it now occurs to me that, if there is no such grouping, providing only @from and @to attributes pointing to the timeline may not be enough to identify the annotated part of the transcription, because (in the case of simultaneous speech) there may be more than one candidate. An alternative to making the grouping obligatory would be to provide a @who element in &lt;spanGrp&gt; or &lt;span&gt;, but this is not allowed according to the guidelines.</hi></p><p>The use of further annotation techniques (e.g. via feature structures) is not precluded, but not in the scope of this document.</p><table rend="rules"><row><cell><p><hi rend="color(333333)">&lt;!-- part-of -speech annotations --&gt;</hi></p><p><hi rend="color(333333)">&lt;!-- using a reference to ids of &lt;w&gt; elements --&gt;</hi></p><p><hi rend="color(333333)">&lt;spanGrp type="pos"&gt;</hi></p><p><hi rend="color(333333)">   &lt;span from="#w148" to="#w148"&gt;PersPron&lt;/span&gt;</hi></p><p><hi rend="color(333333)">&lt;/spanGrp&gt;</hi></p><p><hi rend="color(333333)">&lt;!-- annotations from a sup (=suprasegmentals) tier --&gt;</hi></p><p><hi rend="color(333333)">&lt;!-- using a reference to the timeline --&gt;</hi></p><p><hi rend="color(333333)">&lt;spanGrp type="sup"&gt;</hi></p><p><hi rend="color(333333)">   &lt;span from="#T2" to="#T4"&gt;faster&lt;/span&gt;</hi></p><p><hi rend="color(333333)">&lt;/spanGrp&gt;</hi></p><p><hi rend="color(333333)">&lt;!-- annotations from an en (=English translation) tier --&gt;</hi></p><p><hi rend="color(333333)">&lt;!-- using a reference to the timeline --&gt;</hi></p><p><hi rend="color(333333)">&lt;spanGrp type="en"&gt;</hi></p><p><hi rend="color(333333)">   &lt;span from="#T1" to="#T2"&gt;Okay. &lt;/span&gt;</hi></p><p><hi rend="color(333333)">   &lt;span from="#T2" to="#T4"&gt;Very good, very good.&lt;/span&gt;</hi></p><p><hi rend="color(333333)">&lt;/spanGrp&gt;</hi></p></cell></row></table></div><div xml:id="h.80clzsvnajnb"><head>6.5 Grouping of utterances and dependent annotations (&lt;annotatedU&gt;)</head><p>&lt;u&gt; elements and the annotations referring to it can be grouped under a &lt;annotatedU&gt; element. This has the advantage of creating “local” annotated environments each (succession) of which can be treated as an independent transcription in its own right (“tesselation” of the transcription document)</p><table rend="rules"><row><cell><p><hi rend="color(333333)">&lt;annotatedU&gt;</hi></p><p><hi rend="color(333333)">   &lt;!-- the transcribed text from the primary tier --&gt;</hi></p><p><hi rend="color(333333)">   &lt;u who="#SPK0"&gt;</hi></p><p><hi rend="color(333333)">         &lt;!-- [...] (see above) --&gt;</hi></p><p><hi rend="color(333333)">   &lt;/u&gt;</hi></p><p><hi rend="color(333333)">   &lt;!-- additional annotations from a sup (=suprasegmentals) tier --&gt;</hi></p><p><hi rend="color(333333)">    &lt;spanGrp type="sup"&gt;</hi></p><p><hi rend="color(333333)">         &lt;!-- [...] (see above) --&gt;</hi></p><p><hi rend="color(333333)">    &lt;/spanGrp&gt;</hi></p><p><hi rend="color(333333)">    &lt;!-- additional annotations from an en (=English translation) tier --&gt;</hi></p><p><hi rend="color(333333)">    &lt;spanGrp type="en"&gt;</hi></p><p><hi rend="color(333333)">         &lt;!-- [...] (see above) --&gt;</hi></p><p><hi rend="color(333333)">    &lt;/spanGrp&gt;</hi></p><p><hi rend="color(333333)">&lt;/div&gt;</hi></p></cell></row></table></div><div xml:id="h.dfbr5b22grbz"><head>6.6 Independent elements outside utterances</head><p>&lt;pause&gt; and &lt;incident&gt; elements should be used to represent non-verbal phenomena which cannot be attributed to a speaker. In the document, these elements appear on the same hierarchical level as &lt;annotatedU&gt; elements. In order to fit them into the temporal structure they must have @start and @end attributes pointing to the timeline.</p><table rend="rules"><row><cell><p>&lt;annotatedU&gt;</p><p>     &lt;!-- [...] u and spanGrp elements, see above --&gt;</p><p>&lt;/annotatedU&gt;</p><p>&lt;!-- an incident from a nv (=nonverbal) tier describing nonverbal behaviour --&gt;</p><p>&lt;incident who="#SPK0" type="nv" start="#T3" end="#T6"&gt;</p><p>  &lt;desc&gt;right hand raised&lt;/desc&gt;</p><p>&lt;/incident&gt;</p><p>&lt;annotatedU&gt;</p><p>     &lt;!-- [...] u and spanGrp elements, see above --&gt;</p><p>&lt;/annotatedU&gt;</p></cell></row></table></div></div><div xml:id="h.cch0wsj32244"><head>7 Microstructure</head><div xml:id="h.lxgbc8fwubin"><head>7.1 Words</head><div xml:id="h.aqf1hmp37zy"><head>7.1.1 Characterisation</head><p>Most transcription conventions do not provide an exact and comprehensive definition of the unit <hi rend="italic">word</hi>. Rather, they depart from the word definition of standard written orthography and supplement this with rules for a selected number of special cases (e.g. words specific to spoken language like ‘ehm’, abbreviations, spellings etc.). A more precise definition should and need not be attempted in this document - the decision of what is to be treated (i.e. marked up) as a word can be left to the individual transcription system. → Simple Analytic Mechanisms</p><p>Some transcription conventions have methods for representing an <hi rend="italic">assimilation</hi>, i.e. a blending, of two or more words into one. Also common are methods for characterising a word as <hi rend="italic">incomplete </hi>(cut-off or initialising a self repair sequence). Where certain syllables are pronounced more lengthened than in standard pronunciation, some transcription conventions mark this syllable accordingly. </p><p>pause inside w </p><p>trunc element (speech management) &lt;gap&gt;???</p></div><div xml:id="h.1eypzalj545n"><head>7.1.2 Representation as &lt;w&gt;</head><p>Words (as defined by the transcription system used) should be encoded as &lt;w&gt; elements underneath a &lt;u&gt; element. In order to make words referenceable in annotations, the use of an @id attribute is recommended. A @type attribute can be used to represent special features of a word, especially when the corresponding distinction is an integral part of the transcription system. For instance, the following distinctions made by several widely used transcription systems can be encoded in a @type attribute of a &lt;w&gt; element:</p><list type="unordered"><item>@type=’assimilated’ on the later word for assimilated words</item><item>@type=’truncated’ for truncated words</item><item>@type=’repetition’ for repeated words</item><item>???here new ‘restart’???</item></list><p>Alternatively, this kind of information can be recorded as an annotation in a &lt;span&gt; element (see above).</p><p>Beneath the level of words, many transcription conventions contain instructions for marking a given syllable as accentuated or a given sound as lengthened. To delimit such units below the word level, a &lt;seg&gt; element can be used and either be characterised as an accentuated syllable or lengthened sound by an appropriate @type attribute or, again, by referencing the &lt;seg&gt; element from a &lt;span&gt; via its @id attribute.</p><p>Pauses inside words can occur and should be encoded as &lt;pause&gt; elements as described below.</p></div><div xml:id="h.7kfy6iwvrix9"><head>7.1.3 Further constraints</head><p>Since overlaps starting or ending inside a word occur, &lt;w&gt; must allow &lt;anchor&gt; as a child.</p></div><div xml:id="h.sdziubp2fac9"><head>7.1.4 Examples</head><table rend="rules"><row><cell><p>&lt;!-- an utterance divided into words --&gt;</p><p>&lt;u&gt;</p><p>&lt;!-- [...] --&gt;</p><p>   &lt;w xml:id="w148"&gt;I&lt;/w&gt;</p><p>   &lt;w xml:id="w149"&gt;am&lt;/w&gt;</p><p>   &lt;w xml:id="w150"&gt;very&lt;/w&gt;</p><p>   &lt;w xml:id="w151"&gt;much&lt;/w&gt;</p><p>   &lt;w xml:id="w152"&gt;aware&lt;/w&gt;</p><p>   &lt;w xml:id="w153"&gt;of&lt;/w&gt;</p><p>   &lt;w xml:id="w154"&gt;that&lt;/w&gt;</p><p>&lt;/u&gt;</p><p>&lt;!-- a word with a time anchor inside --&gt;</p><p>&lt;w xml:id="w152"&gt;a&lt;anchor synch="#T3"/&gt;ware&lt;/w&gt;</p><p>&lt;!-- a word with an accentuated syllable --&gt;</p><p>&lt;w xml:id="w152"&gt;&lt;seg xml:id="seg152a"/&gt;awe&lt;/seg&gt;some&lt;/w&gt;</p><p>&lt;span from="seg152a" to="seg152a"&gt;accentuated&lt;/span&gt;</p></cell></row></table></div></div><div xml:id="h.z3o3yr42f496"><head>7.2 Pauses</head><div xml:id="h.x2cshg87ru4t"><head>7.2.1 Characterisation</head><p>Most transcription systems distinguish measured pauses and typed pauses, the latter being typically divided into four types like ‘micro’, ‘short’, ‘medium’ and ‘long’. Pauses can occur outside speaker’s utterances and between or inside words attributed to a &lt;u&gt; element.</p></div><div xml:id="h.33gkhoe4mvlj"><head>7.2.2 Representation as &lt;pause&gt;</head><p>All pauses should be represented as &lt;pause&gt; elements. For measured pauses, the length should be provided in a @dur attribute, for typed pauses, the type should be provided in a @type attribute.</p></div><div xml:id="h.b8a9llpufup4"><head>7.2.3 Further constraints</head><p>Temporal plausibility - length of measured pause should not contradict temporal information as encoded in timeline references</p></div><div xml:id="h.1yclhttdt8hs"><head>7.2.4 Examples</head><table rend="rules"><row><cell><p>&lt;!-- measured pause --&gt;</p><p>&lt;pause dur="PT1.2S"/&gt;</p><p>&lt;!-- typed pause --&gt;</p><p>&lt;pause type="micro"/&gt;</p><p>&lt;!-- measured pause outside &lt;u&gt;, with its own start and end attributes --&gt;</p><p>&lt;pause dur="PT0.61S" start="TLI_10" end="TLI_11"/&gt;</p></cell></row></table></div></div><div xml:id="h.bbkr4lki56qj"><head>7.3 Audible non-speech events </head><div xml:id="h.bplr5t6cwkf"><head>7.3.1 Characterisation</head><p>breathing, laughing, coughing etc.</p><p>noises not attributable to a speaker (e.g. telephone rings)</p><p>visible non-speech events (e.g. nods)? → multimodal, not in the scope of this document?</p></div><div xml:id="h.jk6li6g4qn6v"><head>7.3.2 Representation as &lt;vocal&gt;, &lt;kinesic&gt; or &lt;incident&gt;,</head></div><div xml:id="h.9lqsmgrbsjcc"><head>7.3.3 Further constraints</head><p>order of contributions and elements on the same level:</p><list type="ordered"><item>ascending by position of @start in &lt;timeline&gt;</item><item>descending by position of @end in &lt;timeline&gt; (when @start are equal)</item><item>ascending by position of @who in &lt;particDesc&gt; (when @start and @end are equal)</item></list></div><div xml:id="h.o0t7l0xrs9ed"><head>7.3.4 Examples</head><table rend="rules"><row><cell><p><hi rend="color(333333)">&lt;!-- coughing encoded as incident between words and anchors of a u --&gt;</hi></p><p><hi rend="color(333333)">&lt;u&gt;</hi></p><p><hi rend="color(333333)">    &lt;anchor synch="#T4"/&gt;</hi></p><p><hi rend="color(333333)">    &lt;w&gt;dépend&lt;/w&gt;</hi></p><p><hi rend="color(333333)"> </hi><hi rend="bold color(333333)">&lt;incident&gt;</hi></p><p><hi rend="bold color(333333)">       &lt;desc&gt;cough&lt;/desc&gt;</hi></p><p><hi rend="bold color(333333)">     &lt;/incident&gt;</hi></p><p><hi rend="color(333333)">    &lt;anchor synch="#T5"/&gt;</hi></p><p><hi rend="color(333333)">    &lt;w&gt;un&lt;/w&gt;</hi></p><p><hi rend="color(333333)">    &lt;w&gt;peu&lt;/w&gt;</hi></p><p><hi rend="color(333333)">    &lt;anchor synch="#T6"/&gt;</hi></p><p><hi rend="color(333333)">  &lt;/seg&gt;</hi></p><p><hi rend="color(333333)">&lt;/u&gt;</hi></p></cell></row></table></div></div><div xml:id="h.vdzrkhs7l0s2"><head>7.4 Punctuation</head><div xml:id="h.2xy9getlagg5"><head>7.4.1 Characterisation</head><p>punctuation according to orthography (e.g. period at the end of a grammatical sentence or comma introducing a subordinate clause in German) vs. puncutation representing properties of speech (e.g. comma representing a rising tone movement, slash representing the initialisation of a repair sequence) - the latter should ideally be mapped to a corresponding annotation element, but this is not always feasible</p><p>N.B.: In contrast to other elements, punctuation does usually not directly correspond to some event occuring in time → no start and end attribute possible</p></div><div xml:id="h.wbf984n48hgc"><head>7.4.2 Representation as &lt;pc&gt;</head></div><div xml:id="h.h0bz0y1zdq7z"><head>7.4.3 Further constraints</head></div><div xml:id="h.bm4hbmm13f0v"><head>7.4.4 Examples</head><table rend="rules"><row><cell><p>&lt;!-- punctuation represented as pc elements --&gt;</p><p>&lt;seg function="utterance"&gt;</p><p>&lt;w xml:id="w330"&gt;No&lt;/w&gt;</p><p>&lt;pc&gt;,&lt;/pc&gt;</p><p>&lt;w xml:id="w331"&gt;I&lt;/w&gt;</p><p>&lt;w xml:id="w332"&gt;mean&lt;/w&gt;</p><p>&lt;w xml:id="w333"&gt;I&lt;/w&gt;</p><p>&lt;w xml:id="w334"&gt;knew&lt;/w&gt;</p><p>&lt;pc&gt;.&lt;/pc&gt;</p><p>&lt;/seg&gt;</p></cell></row></table></div></div><div xml:id="h.19xwnk4qpcrs"><head>7.5 Uncertainty and incomprehensible passages</head><div xml:id="h.zigkq2ao9brr"><head>7.5.1 Characterisation</head></div><div xml:id="h.9qjwz3kvltva"><head>7.5.2 Representation as &lt;unclear&gt; </head><p>&lt;unclear&gt; with PCDATA represents uncertainty</p><p>alternatives should be each marked as unclear and grouped with a &lt;choice&gt; element</p><p>completely uncomprehensible passages should be represented by a &lt;gap&gt; element. &lt;gap reason=”passing truck”/&gt;</p></div><div xml:id="h.3qi69r7pwi08"><head>7.5.3 Further constraints</head></div><div xml:id="h.8jyhcow71dsl"><head>7.5.4 Examples</head><table rend="rules"><row><cell><p><hi rend="color(333333)">&lt;!-- uncertain passage --&gt;</hi></p><p><hi rend="color(333333)">&lt;w&gt;you&lt;/w&gt;</hi></p><p><hi rend="color(333333)">&lt;unclear&gt;</hi></p><p><hi rend="color(333333)">  &lt;w&gt;should&lt;/w&gt;</hi></p><p><hi rend="color(333333)">&lt;/unclear&gt;</hi></p><p><hi rend="color(333333)">&lt;w&gt;let&lt;/w&gt;</hi></p><p><hi rend="color(333333)">&lt;!-- uncertain passage with alternatives --&gt;</hi></p><p><hi rend="color(333333)">&lt;w&gt;you&lt;/w&gt;</hi></p><p><hi rend="color(333333)">&lt;choice&gt;</hi></p><p><hi rend="color(333333)"> &lt;unclear&gt;</hi></p><p><hi rend="color(333333)">   &lt;w&gt;should&lt;/w&gt;</hi></p><p><hi rend="color(333333)"> &lt;/unclear&gt;</hi></p><p><hi rend="color(333333)"> &lt;unclear&gt;</hi></p><p><hi rend="color(333333)">   &lt;w&gt;could&lt;/w&gt;</hi></p><p><hi rend="color(333333)"> &lt;/unclear&gt;</hi></p><p><hi rend="color(333333)">&lt;/choice&gt;</hi></p><p><hi rend="color(333333)">&lt;w&gt;let&lt;/w&gt;</hi></p></cell></row></table></div></div></div><div xml:id="h.gvsavu68whzo"><head>7.6 Units above the word and below the &lt;u&gt; level</head><div xml:id="h.sv2zrql6ctw5"><head>7.6.1 Characterisation</head><p>Speaker’s contributions can often be subdivided in chunks comprising more than one word and/or pauses and/or non-audible speech events. These are the “sentence equivalents” of spoken language.</p><p>How these chunks are defined, distinguished and delimited varies greatly between different conventions (and is hotly debated). Two popular approaches: use pragmatic/syntactic criteria (--&gt; notion of an utterance, e.g. HIAT/CHAT) vs. use prosodic critertia (--&gt; notion of an Intonation phrases, e.g. GAT, DT)</p><p rend="heading 3"><anchor xml:id="h.87orfhd8c8ry"/>7.6.2 Representation as &lt;seg&gt; </p><p rend="heading 3"><anchor xml:id="h.vyllcplp9hbj"/>7.6.3 Further constraints</p><p rend="heading 3"><anchor xml:id="h.1u333zjazr4s"/>7.6.4 Examples</p><table rend="rules"><row><cell><p>&lt;!-- u divided into two seg elements --&gt;</p><p>&lt;div&gt;</p><p>&lt;u who="#SPK0"&gt;</p><p>&lt;anchor synch="#T40"/&gt;</p><p>&lt;seg function="utterance" type="declarative"&gt;</p><p>&lt;w xml:id="w319"&gt;And&lt;/w&gt;</p><p>&lt;incident&gt;</p><p>&lt;desc&gt;unv.&lt;/desc&gt;</p><p>&lt;/incident&gt;</p><p>&lt;w xml:id="w320"&gt;disappointed&lt;/w&gt;</p><p>&lt;w xml:id="w321"&gt;when&lt;/w&gt;</p><p>&lt;w xml:id="w322"&gt;you&lt;/w&gt;</p><p>&lt;w xml:id="w323"&gt;got&lt;/w&gt;</p><p>&lt;w xml:id="w324"&gt;</p><p>to</p><p>&lt;anchor synch="T41"/&gt;</p><p>gether</p><p>&lt;/w&gt;</p><p>&lt;/seg&gt;</p><p>&lt;anchor synch="T42"/&gt;</p><p>&lt;seg function="utterance" type="interrogative"&gt;</p><p>&lt;incident&gt;</p><p>&lt;desc&gt;unv.&lt;/desc&gt;</p><p>&lt;/incident&gt;</p><p>&lt;w xml:id="w325"&gt;you&lt;/w&gt;</p><p>&lt;pc&gt;,&lt;/pc&gt;</p><p>&lt;w xml:id="w326"&gt;Victoria&lt;/w&gt;</p><p>&lt;/seg&gt;</p><p>&lt;anchor synch="#T43"/&gt;</p><p>&lt;/u&gt;</p><p>&lt;/div&gt;</p></cell></row></table><p><pb/></p></div></div><div xml:id="h.mr3zkmijs0p1"><head>8 Bibliographical reference</head><p><hi rend="bold">Bird, S. &amp; Liberman, M. (2001).</hi> A formal framework for linguistic annotation. In: Speech Communication (33), 23-60.</p><p><hi rend="bold">Romary Laurent, Witt Andreas (2012)</hi>. Data formats for phonological corpora. Handbook of Corpus Phonology Oxford University Press (Ed.) [<ref target="http://hal.inria.fr/inria-00630289"><hi rend="color(363842)">http://hal.inria.fr/inria-00630289</hi></ref>] </p><p><hi rend="bold">Schmidt, Thomas (2005)</hi> Computergestützte Transkription - Modellierung und Visualisierung gesprochener Sprache mit texttechnologischen Mitteln. Frankfurt a. M.: Peter Lang. </p><p><hi rend="bold">Schmidt, Thomas (2011).</hi> A TEI-based Approach to Standardising Spoken Language Transcription. Journal of the Text Encoding Initiative [Online], Issue 1 | June 2011, URL : [<ref target="http://jtei.revues.org/142"><hi rend="underline color(1155cc)">http://jtei.revues.org/142] </hi></ref>; DOI : 10.4000/jtei.142</p><p><hi rend="bold">Schmidt, T.; Duncan, S.; Ehmer, O.; Hoyt, J.; Kipp, M.; Magnusson, M.; Rose, T. &amp; Sloetjes, H. (2009)</hi> An Exchange Format for Multimodal Annotations. In: Michael Kipp, Jean-Claude Martin, P. P. &amp; Heylen, D. (eds.): Multimodal Corpora, Lecture Notes in Computer Science 207-221. Springer. </p><p><hi rend="bold">Schmidt, Thomas, Elenius, Kjell &amp; Trilsbeek, Paul (2010).</hi> Multimedia Corpora (Media encoding and annotation). Draft submitted to CLARIN WG 5.7. as input to CLARIN deliverable D5.C-3 "Interoperability and Standards" [<hi rend="underline color(1155cc)">http://www.clarin.eu/system/files/clarin-deliverable-D5C3_v1_5-finaldraft.pdf]</hi></p><p>see also: <ref target="http://www1.uni-hamburg.de/exmaralda/files/CLARIN_Standards.pdf"><hi rend="underline color(1155cc)">http://www1.uni-hamburg.de/exmaralda/files/CLARIN_Standards.pdf</hi></ref></p></div><div xml:id="h.sxruogfa10rl"><head/><p><pb/></p></div><div xml:id="h.wn7z4lufnxr7"><head>9 Annex</head><p>ODD spec</p></div><div xml:id="h.yaezeltxcvgx"><head>10 Annex - fully encoded example</head></div><div xml:id="h.y08vdpyda3nq"><head>11 Annex(es)</head><p>Mappings - macrostructure</p></div><div xml:id="h.t304ji4idlqo"><head>12 Annex(es)</head><p>Mappings - microstructure</p><div xml:id="h.lw3w0tgd5m6q"><head>Minutes ISO/DIN Meeting Berlin, 22-October-2012</head><p>(Lou Burnard, see also <ref target="http://www.tei-c.org/Activities/Council/Working/tcw25.xml"><hi rend="underline color(1155cc)">http://www.tei-c.org/Activities/Council/Working/tcw25.xml</hi></ref>)</p><p><hi rend="color(222222) background(white)">EIT MMI Meeting, Berlin 22 oct 2012<lb/><lb/>As noted at the last FTF, Laurent Romary in his capacity as ISO TC7 WG3 chair has proposed a new ISO/TEI joint activity in the area of speech transcription, which comes with the slightly obscure label of EIT MMI: the last part of which is short for “multimodal interaction”, although it seems the activity is really only concerned with speech transcription. I was invited to attend the third EIT MMI workshop, held at the DIN's offices in Berlin. </hi></p><p><hi rend="color(222222) background(white)">Prime movers in the activity, apart from Laurent, appear to be Thomas Schmidt and Andreas Witt from the Institut fur Deutsche Sprache in Mannheim, but a number of other European research labs, mostly concerned with analysis of corpora of human computer interaction, were also represented; specifically: Nadia Mana from FBK (Trento, Italy); Tatjana Scheffler (DFKI, Germany); Khiet Truong (Univ of Twente) ; Benjamin Weiss (TU Berlin); Mathias Wilhelm (DAI Labor); Bertrand Gaiffe (ATILF, Nancy). This being an ISO activity, the real world of commerce and industry was also represented by Felix Burkhardt from Deutsche Telekom's Innovation Lab.<lb/>Related ISO activity mentioned by Laurent included the work on Discourse Relations led by Harry Bunt, and the long-awaited MAF (morpho-syntactic annotation framework) which are both due to appear Real Soon Now. A quick tour de table confirmed my impression that most of the attendees were primarily researchers in Human Computer Interaction with little direct experience of the construction or encoding of spoken corpora, but Thomas Schmidt more than made up for that. </hi></p><p><hi rend="color(222222) background(white)">The main business of the day was to go through his preliminary draft working document, the objective of which is to confer ISO authority on a subset of the existing TEI proposals for spoken text transcription, with some possible modification. The underlying work is well described in Schmidt's recent excellent article in TEIJ, so I won't repeat it: essentially, it consists of a close look at the majority of transcription formats used by the relevant research community/ies and tools, a synthesis of what they have in common, and suggestions of how that synthesis maps to TEI. This is to a large extent motivated by concerns about preservation and migration of data in “legacy” formats.<lb/><lb/>The discussion began by establishing boundaries: despite my proposal to the contrary, it seems there was little appetite to extend the work into the area of truly multimodal transcriptions, which was still generally felt to be insufficiently understood for a practice-based standard to be appropriate. Concern was expressed that we should not make ad hoc premature suggestions. So the document really only concerns transcribed <lb/>speech. There was no disagreement with the general approach which is to distinguish a small number of macro-structural features provide guidelines about how to mark up specific units of analysis at the micro-structural level, using a subset of the TEI. I was also much cheered by two further remarks he made the graph-based “annotation framework” formalisation proposed by Bird and Liberman was theoretically complete but so generic as to be practically useless (I paraphrase) at the micro level, everything you need is there in the TEI (I quote)<lb/><lb/>Discussion focussed on the following points raised by the working document:<lb/></hi></p><list type="unordered"><item><hi rend="bold color(222222) background(white)">Tiers: </hi><hi rend="color(222222) background(white)">Many existing tools organise transcriptions into “tiers” of annotation. <lb/>These seem to be purely technical artefacts, which can be addressed more <lb/>exactly by used of XML markup. Unlike “levels” of annotation, they have <lb/>no semantics. It's doubtful that we need a &lt;tier&gt; element.</hi></item></list><list type="unordered"><item><hi rend="bold color(222222) background(white)">Metadata -1:</hi><hi rend="color(222222) background(white)"> How many of the (very rich) TEI proposals should be included, or mentioned? And how should the three things Thomas had found missing be supplied? I suggested that &lt;appinfo&gt; was an appropriate way to record information about the transcription tool used; that the definition of the transcription system used belonged in the &lt;encodingDesc&gt;; and agreed that there was nothing specifically provided for recording pointers or links to the original video or audio transcribed. In the meeting, I <lb/>speculated that maybe there was scope for extending (or misusing) &lt;facsimile&gt; for this last purpose; another possibility which occurs to me as I type these notes is that one could also extend &lt;recordingDesc&gt;.</hi></item><item><hi rend="bold color(222222) background(white)">Timing: </hi><hi rend="color(222222) background(white)">The timeline is fundamental to the macrostructure of a transcript. Thomas' examples all used absolute times for its &lt;when&gt;s, but I suggested that relative ones might be easier. The document ordering both of &lt;when&gt;s and of transcribed speech should reflect the temporal order as far as possible; this would allegedly facilitate interoperability</hi></item><item><hi rend="bold color(222222) background(white)">Metadata-2: </hi><hi rend="color(222222) background(white)">What metadata was needed, required, recommended for the description of participants? (@sex raised its ugly head here). Could we use &lt;person&gt; to refer to artificial respondents in MMI experiments? (yes, if they have person-like characteristics; no otherwise). <lb/>It was noted that almost any personal trait or state might be crucial to the analysis of some corpora. We noted that CMDI now recommended using the ISOCAT data category registry as an independent way of defining metadata terminology; also that ISOCAT was now available within the TEI scheme (though whether it fits into personal metadata I am less sure). There was (I think) general agreement that we'd reference the various options available in the TEI but not incorporate all of them. We agreed that the principles underlying a given transcription should be clearly documented, either in associated articles, in the formal specification for an encoding, or in the header of individual documents.</hi></item><item><hi rend="bold color(222222) background(white)">Utterances: </hi><hi rend="color(222222) background(white)">Several people disliked the expanded element name &lt;u&gt; and its definition, for various theoretical reasons. Its definition should be modified to remove the implication that it necessarily followed a silence, though we seemed to agree that a &lt;u&gt; could only contain a stretch of speech from a single speaker.<lb/>The temporal alignment of a &lt;u&gt; can be indicated either by @start and @end or by nested &lt;anchor/&gt;s : the standard should probably recommend use of one or the other methods but not both. We discussed whether or not the fact that existing tools did not support the (even simpler) use of @trans to indicate overlap should lead us not to recommend it.</hi></item><item><hi rend="bold color(222222) background(white)">U-plus: </hi><hi rend="color(222222) background(white)">Thomas wanted some method of associating with a &lt;u&gt; the whole block of <lb/>annotations made on it (represented as one or more &lt;interpGrp&gt;s). His document suggested using &lt;div&gt; for this purpose. A lighter-weight solution might be to include &lt;interpGrp&gt; within &lt;u&gt;, or to propose a new wrapper &lt;annotatedU&gt; element.</hi></item><item><hi rend="bold color(222222) background(white)">Tokenization: </hi><hi rend="color(222222) background(white)">Laurent noted that MAF recommended use of &lt;w&gt; for individual tokens; we didn't need to take a stand on the definition of “word” but could simply refer to MAF. We needed some way of signalling the things that older transcription formats had found important, e.g. words considered incomplete, false starts, repetitions, abbreviations etc. so we needed to choose an appropriate TEI construct for them, even if we thought the concept was not useful or ill-defined. The general purpose &lt;seg&gt; element might be the simplest solution, but some diplomacy would be needed about how to define its application and its possible @type or @function values.<lb/></hi></item></list><p><hi rend="bold color(222222) background(white)">Conclusions</hi><hi rend="color(222222) background(white)"><lb/><lb/>This workgroup will probably produce a useful document describing an important use case for the TEI recommendations on spoken language. It is currently a Google Doc which the group has agreed to share with the Council. I undertook to help turn this into an ODD, which could eventually become one of our Exemplars. Work on standardising other aspects of transcribed multimodal interactions probably needs to be deferred to a later stage.</hi></p></div><div xml:id="h.lidhcauvwusg"><head>Comments Carol Etienne (by e-mail 11-February-2013)</head><p><hi rend="color(222222) background(white)">Les membres du groupe de travail  'Interopérabilité' de l'IRCOM, IR Corpus Oraux et Multimodaux, sont intéressés par l'utilisation de la TEI pour les corpus oraux, dans les projets regroupant plusieurs "sources" de corpus ou bien en tant que format d'échange de données entre les logiciels d'annotation comme Transcriber, Clan, Elan ou Anvil majoritairement utilisés dans notre communauté.</hi></p><p><hi rend="color(222222) background(white)">Les besoins en terme de métadonnées comme en terme de transcription sont différents et plus ou moins complexes suivant les objets d'étude de chaque équipe mais les besoins suivants émergent :</hi></p><p><hi rend="color(222222) background(white)">1) des </hi><hi rend="bold color(222222) background(white)">métadonnées pour les locuteurs</hi><hi rend="color(222222) background(white)"> : données sociolinguistiques, relation entre les utilisateurs, langue maternelle et autres langues, langue des parents, situation professionnelle, locuteurs génériques (client/commerçant, hotline, élève/enseignant, ...) La Tei fournit un ensemble important de balises dans &lt;person&gt; qui devraient couvrir la majorité des besoins</hi></p><p><hi rend="color(222222) background(white)">2)  des </hi><hi rend="bold color(222222) background(white)">métadonnées pour l'enregistrement audio ou vidéo</hi><hi rend="color(222222) background(white)"> : type, qualité, anonymisation, url, ...</hi></p><p><hi rend="color(222222) background(white)">La balise &lt;recording&gt; reste à ma connaissance bien pauvre et &lt;equipment&gt; peu formatée</hi></p><p><hi rend="color(222222) background(white)">3)  des </hi><hi rend="bold color(222222) background(white)">métadonnées pour les langues utilisées</hi><hi rend="color(222222) background(white)"> : segments clairement identifiées vs  segments difficiles à catégoriser, code-switching, langues de contact, ...</hi></p><p><hi rend="color(222222) background(white)">Les balises &lt;langUsage&gt; et &lt;language&gt; reposent sur une identification de la  langue , une langue principale mais gèrent assez mal les segments non attribués à moins de définir une typologie de langue ...</hi></p><p><hi rend="color(222222) background(white)">4)  des </hi><hi rend="bold color(222222) background(white)">métadonnées pour identifier la nature de l'oral</hi><hi rend="color(222222) background(white)">, le nombre de locuteurs, ... : récit, enseignement, apprentissage, conversation, … La balise &lt;setting&gt; peut répondre mais elle est peu formatée</hi></p><p><hi rend="color(222222) background(white)">5) dans la transcription, plusieurs niveaux d'annotations sont réalisés et ne correspondent pas forcément au même découpage temporel de cette transcription, le non-verbal n'a par exemple pas les mêmes bornes que le verbal, la traduction s'entend sur un segment de plusieurs mots alors que les lemmes se rapportent à un mot précis, ...</hi></p><p><hi rend="color(222222) background(white)">Je pense que nos collègues travaillant sur le syntaxe seront aussi intéressés par un</hi><hi rend="bold color(222222) background(white)"> jeu d'étiquettes liées au mot</hi><hi rend="color(222222) background(white)">.</hi></p><p><hi rend="color(222222) background(white)">Pour ce point, les balises &lt;spanGrp&gt; et &lt;anchor&gt; de la TEI peuvent répondre à cette question avec une définition de plusieurs bornes (repère temporel ou lien direct à l'id d'un mot ou un anchor "éléctron libre" sans lien avec le timeline ou le mot) pour identifier la portée de ces annotations.</hi></p></div></div></body></text></TEI>